\setcounter{dummy}{16}
\addtocounter{section}{15}
\addtocounter{chapter}{4}

\chapter{Differentiation}
	Differentiation ist lokale Linearisierung.

\section{Wiederholung und Motivation}
	\begin{ueberblick}
	$K^n$ ist ein $n$-dimensionaler VR über dem vollständigen Körper $K=\real$ 
	oder $K=\comp$. Die Elemente sind $x=(x_1,...,x_n)\in K^n$ mit 
	$x_1,...,x_n\in K$. Basis ist die Standardbasis $(e_1,...,e_n)$. \\
	
	
	Alle Normen sind auf $K^n$ äquivalent $\Rightarrow$ Konvergenz ist 
	unabhängig von der Norm. Trotzdem verwenden wir die euklidische Norm: 
	$|x|=\sqrt{\sum\limits_{j=1}^n |x_j|^2}$. \\
	
	
	\underline{Skalarprodukt:} $\langle x,y \rangle=\sum\limits_{j=1}^n x_j\cdot y_j$ in 
	$\real$ bzw. $\langle x,y \rangle=\sum\limits_{j=1}^n \overline{x_j}
	\cdot y_j$ in $\comp$. \\
	
	
	\underline{\person{Cauchy}-\person{Schwarz}-Ungleichung:} $|\langle x,y \rangle| \le
	|x|\cdot |y|$. \\

	
	lineare Abbildung: $A:K^n \to K^m$, Darstellung mittels $m\times n$-Matrix 
	bezüglich Standardbasen in $K^n$ und $K^m$. Beachte: $A$ steht für die 
	lineare Abbildung und die Matrix, die die lineare Abbildung beschreibt. 
	Lineare Abbildungen sind stets stetig (unabhängig von der Norm). Hinweis: 
	$x=(x_1,...,x_n)$ in der Regel als Zeilenvektor geschrieben, aber bei 
	Matrixmultiplikation ist $x$ Spaltenvektor und $x^t$ Zeilenvektor, d.h. \\
	$x^t\cdot y=\langle x,y \rangle$, falls $m=n$ \\
	$x\cdot y^t=x\otimes y$, sogenanntes Tensor-Produkt \\
	
	
	$L(K^n,K^m)=\{A: K^n \to K^m \mid A \text{ linear}\}$ Menge aller 
	linearen Abbildungen mit $||A||=\sup\{|Ax| \mid |x|\le 1\} \to$ Norm 
	hängt im Allgemeinen von Normen auf $K^n$ und $K^m$ ab. \\
	$L(K^n,K^m)$ ist isomorph zu $Mat_{m\times n}(K)$ ist isomorph zu $K^{mn}$ 
	jeweils als VR $\Rightarrow$ $L(K^n,K^m)$ ist ein $m\cot n$-dimensionaler 
	VR $\Rightarrow$ alle Normen sind äquivalent $\Rightarrow$ Konvergenz von 
	$\{A_n\}$ in $L(K^n,K^m)$ unabhängig von Norm, nehme in der Regel statt 
	$||A||$ die euklidische Norm $|A|=\sqrt{\sum\limits_{k=1}^n \sum\limits_
	{l=1}^n |a_{kl}|^2} \Rightarrow$ es gilt: $|Ax|\le ||A||\cdot |x|$ und 
	$|Ax|\le |A|\cdot |x|$. \\
	
	
	Abbildung $f:K^n\to K^m$ heißt affin linear falls $f(x)=Ax+a$ für eine 
	lineare Abbildung $A:K^n\to K^m$. \\
	
	\underline{\person{Landau}-Symbole}: Sei $f:D\subset K^n \to K^m$, $g:D\subset K^n\to K$, 
	$x_0\in \overline{D}$
	\begin{compactitem}
		\item $f(x)=o(g(x))$ für $x\to x_0$ genau dann, wenn $\lim\limits_{\substack{x\to x_0 \\ x\neq x_0}} \frac{|f(x)|}{g(x)}=0$
		\item $f(x)=O(g(x))$ für $x\to x_0$ genau dann, wenn $\exists\delta>0$, $0\le c<\infty$ mit 
		$\frac{|f(x)|}{|g(x)|}\le c \quad \forall x\in (B_{\delta}(x_0)\backslash \{x_0\})\cap D$
	\end{compactitem}
	\end{ueberblick}

	%TODO was soll der wichtige Spezialfall im Skript? Ist der wichtig?

	\begin{beispiel}[$f: D\subset K^n \to K^m$]
		$x_0\in D$, $x_0$ Häufungspunkt von $D$. Dann: $f$ stetig in $x_0 \iff \lim\limits_{x\to x_0} f(x)
		=f(x_0)\iff \lim\limits_{x\to x_0} \frac{|f(x)-f(x_0)|}{1}=0 \iff f(x)=f(x_0)+o(1)$ für $x\to x_0$ \\
		Interpretation: Setze $r(x)=f(x)-f(x_0)\Rightarrow r(x)=o(1)$ für $x\to x_0\Rightarrow r(x)\to 0$, 
		d.h. $o(1)$ ersetzt die Restfunktion $f(x)-f(x_0)$. Wegen $o(1)=o(|x-x_0|^0)$ sagt man auch, 
		dass $f(x)=f(x_0)+o(1)$ die Approximation 0-ter Ordnung der Funktion $f$ in der Nähe von $x_0$.
	\end{beispiel}

	\begin{beispiel}[$f:D\subset \real^n \to \real$]
		$x_0\in D$, $D$ offen, das bedeutet: $f(x)=f(x_0)+o(|x-x_0|)$, $x\to x_0 \quad (*)$
		\begin{compactitem}
			\item betrachte $f$ auf Strahl $x=x_0+ty$, $y\in \real^n$ fest, $|y|=1$, $t\in \real$ \\
			$(*)\Rightarrow 0=\lim\limits_{\substack{x\to x_0 \\ x\neq x_0}} \frac{|f(x)-f(x_0)|}{|x-x_0|}=
			\lim\limits_{\substack{t\to 0 \\ t\neq 0}} \frac{|f(x_0+ty)-f(x_0)|}{|t|} \Rightarrow \frac
			{|\Delta f|}{|t|}=|\text{Anstieg der Sekante}|\to 0$
			\item $(*)\Rightarrow f(x)=f(x_0)+\underbrace{\frac{o(|x-x_0|)}{|x-x_0|}}_{o(1)}|x-x_0|
			\Rightarrow f(x)=f(x_0)+o(1)|x-x_0| \Rightarrow f(x)=f(x_0)+r(x)|x-x_0|\Rightarrow 
			|f(x)-f(x_0)| \le \varrho(t)|x-x_0|$ falls $|x-x_0|\le t$ mit $\varrho(t)=\sup\limits_{|x-x_0|\le t} 
			|r(x)|\to 0$ \\
			Graph von $f$ liegt nahe $x_0$ in "'immer flacheren kegelförmigen Mengen"' $\Rightarrow$ 
			Graph "'schmiegt sich"' an horizontale Ebene durch Punkt $(x_0,f(x_0))$
			\item $(*)$ erfüllt offenbar nicht die Beobachtung: horizontale Ebene ist Graph einer affin 
			linearen Funktion $\tilde A: \real^n \to \real$
		\end{compactitem}
	\end{beispiel}

	\textbf{zentrale Frage:} Gibt es zur Funktion $f:D\subset K^n \to K^m$, $x_0\in D$, eine affin 
	lineare Funktion $\tilde A:K^n\to K^m$, so dass sich in der Nähe von $x_0$ der Graph von $f$ 
	an den Graph von $\tilde A$ "'anschmiegt"'? \\
	\textbf{Antwort:} Ja, wegen $f(x_0)=\tilde A|x_0|$ folgt $\tilde Ax=A(x-x_0)+f(x_0)$
	
	\begin{definition}[Anschmiegen]
		$f(x_0)-(f(x_0)+A(x-x_0))=o(|x-x_0|)$ \\
		d.h. die Abweichung wird schneller kleiner als $|x-x_0|$! \\
		
		Vielleicht hatten Sie bisher eine andere Vorstellung von "'anschmiegen"', aber wir machen hier 
		Mathematik!
	\end{definition}

\section{Ableitung}
	Sei $f:D\subset K^n \to K^m$, $D$ offen.
	
	\begin{definition}[differenzierbar im Punkt]
		$f$ heißt differenzierbar im Punkt $x_0\in D$ falls es eine lineare Abbildung $A\in L(K^n,K^m)$ gibt,
		mit der Eigenschaft $f(x)=f(x_0)+A(x-x_0)+o(|x-x_0|)$ , $x\to x_0$.
	\end{definition}

	\begin{definition}[Ableitung]
		$A$ heißt Ableitung von $f$ an der Stelle $x=x_0$ und wird mit $f'(x_0)=A$ bzw. $Df(x_0)$ 
		bezeichnet. Man kann auch totales Differential, Fréchet-Ableitung, Jacobimatrix oder 
		Funktionalmatrix sagen. \\
		Andere Bezeichnungen sind: $\frac{\partial f}{\partial x}(x_0)$, $\frac{\partial f(x)}{\partial x}\vert_
		{x=x_0}$, d$f(x_0)$,... \\
		Somit gilt: $f(x)=f(x_0)+f'(x_0)(x-x_0)+o(|x-x_0|)$, $x\to x_0$
	\end{definition}

	\begin{bemerkung}
		$f'(x_0)$ ist im Allgemeinen eine von $x_0$ abhängige Matrix! \\
		$\Rightarrow$ lineare Funktion $\tilde A(x)=f(x_0)+f'(x_0)(x-x_0)$ appromximiert Funktion $f$ 
		in der Nähe von $x_0$ und heißt Linearisierung von $f$ in $x_0$.
	\end{bemerkung}

	\begin{satz}
		Sei $f:D\subset K^n \to K^m$, $D$ offen. $f$ ist differenzierbar in $x_0\in D$ mit Abbildung $f'(x_0)\in L(K^n,K^m)$ genau dann, wenn die folgenden Bedingungen erfüllt sind:
		\begin{compactitem}
			\item für ein $r:D\to K^m$ mit $\lim\limits_{\substack{x\to x_0 \\ x\neq x_0}} 
			\frac{r(x)}{|x-x_0|}=0$ \\
			$f(x)=f(x_0)+f'(x_0)(x-x_0)+r(x)\quad\forall x\in D$
			\item für ein $R:D\to L(K^n,K^m)$ mit $\lim\limits_{x\to x_0} R(x)=0$ \\
			$f(x)=f(x_0)+f'(x_0)(x-x_0)+R(x)(x-x_0)\quad\forall x\in D$
			\item für ein $Q:D\to L(K^n,K^m)$ mit $\lim\limits_{x\to x_0} Q(x)=f'(x_0)$ \\
			$f(x)=f(x_0)+Q(x)(x-x_0)\quad\forall x\in D$
		\end{compactitem}
	\end{satz}
	\begin{beweis}
		\begin{compactitem}
			\item offenbar ist $r(x)=o(|x-x_0|)$, $x\to x_0$, folglich ist dies äquivalent zu: $f$ 
			differenzierbar in $x_0$ mit Abbildung $f'(x_0)$
			\item $1\Rightarrow 2$: Sei $R:D\to K^{m\times n}$ gegeben durch $R(x_0)=0$, $R(x)=
			\frac{r(x)}{|x-x_0|^2}\cdot (x-x_0)^t$, $x\neq x_0 \Rightarrow R(x)(x-x_0)=\frac{r(x)}
			{|x-x_0|^2}\langle x-x_0,x-x_0\rangle=r(x)$ \\
			wegen $0=r(x_0)=R(x_0)(x-x_0)$ folgt $r(x)=R(x)(x-x_0)\quad\forall x\in D\Rightarrow 2$ \\
			wegen $|r(x)(x-x_0)^t|=|r(x)||x-x_0|$ folgt $\lim\limits_{x\to x_0} |R(x)|=\lim\limits_
			{\substack{x\to x_0 \\ x\neq x_0}} \frac{|r(x)\cdot (x-x_0)^t|}{|x-x_0|^2}=\lim\limits_
			{\substack{x\to x_0 \\ x\neq x_0}} \frac{|r(x)||x-x_0|}{|x-x_0|^2}=0\Rightarrow 2$
			\item $2\Rightarrow 3$: setze $Q(x)=f'(x_0)+R(x)\quad\forall x\in D\Rightarrow 3$ \\
			wegen $\lim\limits_{x\to x_0} Q(x)=f'(x_0)$ folgt 3
			\item $3\Rightarrow 1$: setze $r(x)=(Q(x)-f'(x))(x-x_0)$ wegen $|r(x)|\le |Q(x)-f'(x)||x-x_0|$ 
			folgt $\lim\limits_{\substack{x\to x_0 \\ x\neq x_0}} \frac{|r(x)|}{|x-x_0|}=\lim\limits_{x\to x_0} 
			|Q(x)-f'(x_0)|=0\Rightarrow$ Definition Ableitung
		\end{compactitem}
	\end{beweis}

	\begin{satz}
		Sei $f:D\subset K^n \to K^m$, $D$ offen, $f$ differenzierbar in $x_0\in D$. Dann:
		\begin{compactitem}
			\item $f$ ist stetig in $x_0$.
			\item Ableitung $f'(x_0)$ ist eindeutig bestimmt.
		\end{compactitem}
	\end{satz}
	\begin{beweis}
		\begin{compactitem}
			\item $\lim\limits_{x\to x_0} f(x)=\lim\limits_{x\to x_0} (f(x_0)+f'(x_0)(x-x_0)+R(x)(x-x_0))=f(x_0)
			\Rightarrow$ Behauptung
			\item angenommen $A_1,A_2\in L(K^n,K^m)$ sind Ableitungen von $f$ in $x_0$. Seien $R_1,R_2$ 
			zugehörige Terme. Dann gilt für $x=x_0+ty$: $|(A_1-A_2)(ty)|=|R_1(x_0+ty)(ty)|+|R_2(x_0+ty)
			(ty)| \le |R_1(x+ty)||ty|+|R_2(x_0+ty)||ty|\Rightarrow 0\le |(A_1-A_2)(y)|\le (|R_1(x_0+ty)|+|R_2
			(x_0+ty)|)|y|\to 0\Rightarrow (A_1-A_2)(y)=0\Rightarrow A_1=A_2\Rightarrow$ Behauptung
		\end{compactitem}
	\end{beweis}

\subsection{Spezialfälle für $K=\real$:}
	\begin{compactitem}
		\item $m=1$, $f:D\subset \real^n\to \real$ \\
		$f'(x_0)\in \real^{1\times n}$ ist Zeilenvektor, $f'(x_0)$ betrachtet als Vektor in $\real^n$ heißt auch 
		Gradient. Offenbar $f'(x_0)y=\langle f'(x_0),y\rangle \quad\forall y\in \real^n\Rightarrow f(x)=
		f(x_0)+\langle f'(x_0),x-x_0 \rangle + o(|x-x_0|)$
		\item $n=1$, $f:D\subset \real\to \real^m$, z.B. $D=(a,b)$ \\
		$f$ bzw. Bild $D(f)$ ist Kurve in $\real^m$, $f'(x_0)$ ist Spaltenvektor im $\real^m$. Man kann 
		schreiben: $f(x_0+t)=f(x_0)+tf'(x_0)+o(|t|)$ \\
		$\iff \underbrace{\frac{f(x_0+t)-f(x_0)}{t}}_{\text{heißt Differenzenquotient von
		 }f\text{ in }x_0}=f'(x_0)+o(1)$, $t\to 0\quad \frac{o(t)}{t}=o(1)$ \\
	 	$\iff \underbrace{\lim\limits_{t\to 0}\frac{f(x_0+t)-f(x_0)}{t}}_{\text{heißt Differentialquotient von
	 		}f\text{ in }x_0}=f'(x_0)$ \\
 		\textbf{Bemerkungen:} $f$ differentierbar $\iff$ Diffentialquotient existiert in $x_0$, aber nicht 
 		erklärt für den Fall $n>1$! \\
 		\textbf{Interpretation für $m>1$:} \begin{compactitem}
 			\item Tangente an Kurve: Bild von $\tilde A(\real)$ ist Gerade und heißt Tangente an Kurve 
 			$f(x_0)$
 			\item Tangentenvektor an Kurve in $f(x_0)$ ist $f'(x)$ \\
 			Falls $f$ nicht differenzierbar in $x_0$ bzw. $x_0$ Randpunkt von $D$ und $f(x_0)$ 
 			definiert, betrachtet man einseitige Grenzwerte.
 			\item rechtsseitige Ableitung: $\lim\limits_{t\downarrow 0}\frac{f(x_0+t)-f(x_0)}{t}=f'_r(x_0)$ 
 			heißt rechtsseitige Ableitung von $f$ in $x_0$ (falls existent), analog linksseitige Ableitung
 		\end{compactitem}
 		\item $n=m=1$, $f:D\subset \real\to \real$ \\
 		$f'(x_0)\in \real$ ist Zahl und es gilt: \begin{compactitem}
 			\item Graph von $f$ ist Kurve in $\real^2$
 			\item Graph von $\tilde{A}$ ist Tangente an Graph von $f$ in $(x_0,f(x_0))$ und hat Anstieg 
 			$f'(x_0)$
 		\end{compactitem}
	\end{compactitem}

	\begin{folgerung}
		Sei $f:D\subset K\to K^m$, $D$ offen. Dann: \\
		$f$ ist differenzierbar in $x_0\in D$ mit Ableitung $f'(x_0)\in L(K,K^m)\iff \exists f'(x_0)\in 
		L(K,K^m):\lim\limits_{y\to 0} \frac{f(x_0+t)-f(x_0)}{y}=f'(x_0)$.
	\end{folgerung}

\subsection{einfache Beispiele für Ableitungen}
	\begin{beispiel}[$f:K^n\to K^m$ affin linear]
		Für beliebige $x_0\in K^n$ gilt: $f(x)=Ax_0+a+A(x-x_0)=f(x_0)+A(x-x_0)+0\Rightarrow f$ ist 
		differenzierbar in $x_0$ mit $f'(x_0)=A$
	\end{beispiel}

	\begin{beispiel}[$f:\real^n\to \real$ mit $f(x)=|x|^2$]
		$|x-x_0|^2=\langle x-x_0,x-x_0\rangle=|x|^2-2\langle x_0,x\rangle+2\langle x_0,x_0\rangle-|x_0|^2=
		|x|^2-2\langle x_0,x-x_0\rangle-|x_0|^2\Rightarrow f(x)=f(x_0)+2\langle x_0,x-x_0\rangle+
		\underbrace{|x-x_0|^2}_{o(|x-x_0|)}$ \\
		wegen $2x_0\in L(\real^n,\real)$ folgt $f=|\cdot |^2$ ist differenzierbar in $x_0$ mit $f'(x_0)=2x_0
		\quad\forall x_0\in \real^n$
	\end{beispiel}

	\begin{beispiel}[$f:K\to K$ mit $f(x)=x^k$]
		\begin{compactitem}
			\item $k=0$: $f(x)=1\Rightarrow f'(x)=0$
			\item $k=1$: $f(x_0+y)=(x_0+y)^k=\sum\limits_{j=0}^{k} \binom{k}{j} x_0^{k-j}y^j=x_0^k+
			kx_0^{k-1}y+o(y)=f(x_0)+k\cdot f(x_0)y+o(y)$, $y\to 0\Rightarrow f'(x_0)=kx_0^{k-1} 
			\quad\forall x_0\in K$
		\end{compactitem}
	\end{beispiel}

	\begin{beispiel}[$f:\real^n\to\real$ mit $f(x)=|x|$]
		$f$ ist nicht differenzierbar in $x_0=0$, denn, angenommen Ableitung $f'(0)\in \real^n$ existiert, 
		fixiere $x\in \real^n$ mit $|x|=1\Rightarrow |tx|=0+\langle f'(0),tx\rangle+o(t)$, $t\to 0\Rightarrow
		\frac{|t|}{t}=\langle f'(x),x \rangle + \frac{o(t)}{t}=\pm 1 \Rightarrow$ Widerspruch \\
		anschaulich: es gibt keine Tangentialebene an Graph von $f$ in $(0,|0|)\in \real^{n+1}$ \\
		folglich: $f$ ist stetig in $x_0\not\Rightarrow f$ ist differenzierbar in $x_0$.
	\end{beispiel}

\section{Richtungsableitung und partielle Ableitung}

Sei $f:D \subset K^n \to K^m$, $D$ offen, $x\in D$.\\
\underline{Ziel:} Zurückführung der Berechnung der Ableitung $f^{'}(x)$ auf Berechnung der Ableitung der Funktion $\tilde{f} : \tilde{D} \subset \mathbb{K} \to \mathbb{K}$. Bisher:

\begin{compactitem}
    \item Reduktionsansatz $\Longrightarrow$ man kann sich bereits auf $m=1$ beschränken
    \item für Berechnung der Ableitung von $\tilde{f}$ ist dann neben Rechenregeln auch Differentialquotient (mit leistungsfähigen Grenzwertkalkül!) verfügbar
\end{compactitem}

\underline{Idee:} Betrachte Funktion $f$ auf Gerade $t \to x + tz$ durch $x$ ($z$ Richtungsvektor) $\Longrightarrow$ skalares Argument $t \in \mathbb{K} \longrightarrow$ Differentialquotient\\
\underline{Spezialfälle:} $z=e_j \Rightarrow$ partielle Ableitung
Sei $f:D \subset K^n \to K^m$, $D$ offen, $x \in D$, $z \in K^n$. Falls $a \in \LinAbb(K,K^m)\;(\sim K^m)$ existiert mit
\begin{align}
f(x+tz) = f(x) +ta + o(t), t \to 0, t \in K\\
\end{align}

dann heißt $f$ differenzierbar in $x$ in Richtung $z$ und $\Diff_z f(x):= a$ heißt \begriff{Richtungableitung} $f$ in $x$ in Richtung $z$ andere Bezeichnungen $\partial_z f(x), \frac{\partial f(x)}{\partial_z}(x), \delta f(x;z), f^{'}(x;z), \dots$

\begin{bemerkung}
    \begin{compactitem}
        \item wegen $B_{\epsilon}(x) \subset D$ für ein $\epsilon > 0$ existiert ein $\tilde{\epsilon} > 0$ mit $x+tz \in D \forall t \in B_{\epsilon}(0) \in K$
        \item $\Diff_z f(x)$ existiert offebar stets für $z=0$ mit $\Diff_0 f(x) = 0$
    \end{compactitem}
\end{bemerkung}

\begin{folgerung}
    Sei $f: D \subset K^n \to K^m$, $D$ offen, $x\in D$, $z \in K^n$. Dann
    \begin{align} %TODO Add numbering!
    &f \text{ ist differenzierbar in } x \text{ in Richtung } z \text{ mit } \Diff_z f(x) \in \LinAbb(K, K^m)\;(\sim K^m)\\
    &\Leftrightarrow \text{ für } \varphi(t) = f(x + tz) \text{ existiert } \varphi^{\prime}(0) \text{ und } \Diff_z f(x) = \varphi^{\prime}(0)\\
    &\Leftrightarrow \lim_{t \to 0} \frac{f(x + tz) - f(x)}{t} = a \in \LinAbb(K,K^m) \text{ existiert und } \Diff_z f(x) = a
    \end{align}
\end{folgerung}

\begin{beispiel}
    Sei $f: \mathbb{R}^2 \to \mathbb{R}, f(x_1,x_2) = x_1^2 + \vert x_2\vert$. Existiert Richtungsableitung in $x=(x_1,0)$? Sei $\phi(t) = f(x + tz) = (x_1 + tz_1)^2 + \vert tz_2\vert = \underbrace{x_1^2 + 2tx_1 z_1 +  t^2 z_1^2}_{:= \varphi_1(t)} + \underbrace{\vert t \vert \vert z \vert}_{=: \varphi_2(t)} \Rightarrow \varphi^{\prime}(0) = 2x_1z_1\;\forall x_1, z_1 \in \mathbb{R}, \varphi(\prime)(0) = 0$ existiert \textbf{nur} für $z_2 = 0$ (vgl Bsp 5.1.4) %TODO set ref.\\
    $\Rightarrow \varphi^{\prime}(0) = 2x_1 z_1$ existiert \textbf{nur} für $x_1, z_1 \in \mathbb{R}, z_2 = 0 \overset{5.8}{\Rightarrow}$ Richtungsableitung von $f$ existiert für alle $x=(x_1,0)$ \textbf{nur} in Richtung $z=(z_1,0)$ mit $\Diff_z f(x) = 2x_1 z_1$
\end{beispiel}

\underline{Frage:} Existiert $\Diff_z f(x)\; \forall z$ falls $f$ differenzierbar in $x$?

\begin{satz}\label{satz:Richtungsableitung_linear}
    Sei $f : D \subset K^m \to K^m$, $D$ offen, $f$ differenzierbar in $x \in D \Rightarrow$ Richtungsableitung $\Diff_z f(x)$ existiert $\forall z \in K^n$ und 
    \begin{align}
    \Diff_z f(x) = f^{\prime}(x)\cdot z
    \end{align}
    \textit{Bemerkung:}\\
    Richtungableitung ist \textbf{linear} in $z$!
\end{satz}

\begin{proof}
    $f$ differenzierbar in $x \Rightarrow f(y) = f(x) + f^{\prime}(x)(y-x) + o(\vert y-x\vert) \overset{y=x+tz}{\Rightarrow} f(x+tz) = f(x) t(f^{\prime}(x)z + o(t), t \to 0, y \to x \overset{5.7}{\Rightarrow}$ Behauptung.
\end{proof}

\begin{beispiel}
    Betrachte $f: \mathbb{R}^n \to \mathbb{R}$ mit $f(x) = \vert x \vert^2$
    \begin{compactitem}
        \item[a)] (5.8) liefert $\varphi(t) = \vert x + tz\vert^2 = \sum_{i=1}^{n} (x_i t z_i)^2 = \sum_{i=1}^{n} x_i^2 + 2t x_i z_i + t^2 z_i^2 \Rightarrow \varphi^{\prime}(t) = \sum_{i=1}^{n} 2x_i z_i + 2t z_i^2 \overset{(5.8)}{\Rightarrow} \varphi^{\prime}(0) = 2 \sum_{i=1}^{n} x_i z_i = \langle x,z \rangle = \Diff_z  f(x)\; \forall x,z \in \mathbb{R}^n$
        \item[b)] Beispiel 5.1.2 liefert $f^{\prime}(x) = 2x \forall x \in \mathbb{R}^n \overset{(5.10)}{\Rightarrow} \Diff_z f(x) = 2x \cdot z = 2 \langle x,z \rangle \forall x,z \in \mathbb{R}^n$ folglich gilt für $\vert z \vert = 1$ und $x \in \mathbb{R}^n$ fest:
        \begin{compactitem}[\textbullet]
            \item $\Diff_z f(x) \Longleftrightarrow x \perp z$
            \item $\Diff_z f(x)$ maximal $\Longleftrightarrow z = \frac{x}{\vert x \vert}$
        \end{compactitem}
    \end{compactitem}
\end{beispiel}

\subsection{Anwendung: Eigenscht des Gradienten}

Sei $f: D \subset \mathbb{R}^n \to \mathbb{R}$, $D$ offen, $f$ differenzierbar in $x\in D$.\\

\begin{definition}
    $N_c := \{x \in D \mid f(x) = c\}$  heißt \begriff{Niveaumenge} von f.\\
    Sei $\gamma := (-\delta, \delta) \to N_c(\delta > 0)$ Kurve mit $\gamma(0) = x$ (*), $\gamma$ differenizierbar in $0$.\\
    Ein $z \in \mathbb{R}^n\setminus\{0\}$  mit $z = \gamma^{\prime}$ für eine Kurve $\gamma$ gemäß (*) heißt \begriff{Tangentialvektor} an $N_c$ in $x_0$.
\end{definition}

Offenbar $\varphi(t) := f(\gamma(t)) = c \forall t \in (-\delta, \delta)\\ \overset{Kettenregel}{\Longrightarrow} \varphi^{\prime}(0) = f^{\prime}(\gamma(0))\cdot \gamma^{\prime}(0) = 0 \overset{Satz \ref{satz:Richtungsableitung_linear}}{\Longrightarrow} \Diff_{\gamma^{\prime}(0)}f(x) = \langle f^{\prime}(x),\gamma^{\prime}(0)\rangle =0$ (**)

\begin{satz}
    Sei $f: D \subset \mathbb{R}^n \to \mathbb{R}$, $D$ offen, $f$ differenzierbar in $x \in D$. Dann
    \begin{compactitem}
        \item[1)] Gradient $f^{\prime}(x)$ steht senkrecht auf Niveaumenge $N_{f(x)}$, d.h. $\langle f^{\prime}(x), z\rangle = 0 \forall$ Tangentialebenen $z$ an $N_{f(x)}$ in $x$.
        \item[2)] Richtungsableitung $\Diff_z f(x) = 0 \forall$ Tangentialebenen $z$ an $N_{f(x)}$ in $x$.
        \item[3)] Gradient $f^{\prime}(x)$ zeigt in Richtung des ``steilsten Anstiegs'' von $f$ in $x$, d.h. falls $f^{\prime}(x) \neq 0$ gilt für die Richtung $\bar{z} = \frac{f(x)}{\vert f(x) \vert} \Diff_z f(x) = \max\{\Diff_z f(x) \in \mathbb{R} \mid z \in \mathbb{R}^n \text{ und } \vert z \vert = 1\} = \vert f^{\prime}(x)\vert.$
    \end{compactitem}
\end{satz}

\begin{proof}
    1), 2) folgen direkt aus (**) und 5.10\\
    zu 3) für $\vert z \vert = 1$ gilt $\Diff_z f(x) \overset{5.10}{=} \langle f^{\prime}(x), z \rangle$ $\overset{\text{Def. } \bar{z}}{=} \vert f^{\prime}(x)\vert \langle \tilde{z}, z\rangle \overset{\text{CSU}}{\leq} \vert f^{\prime}(x)\vert \vert \tilde{z}\vert \vert z\vert = \vert f^{\prime}(x)\vert = \frac{\langle f^{\prime}(x),f^{\prime}(x)}{\vert f^{\prime}(x) \vert} = \langle f^{\prime}(x), \tilde{z}\rangle \Diff_z f(x) \Rightarrow$ Behauptung. %TODO validate proof!
\end{proof}