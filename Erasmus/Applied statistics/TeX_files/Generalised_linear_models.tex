So far, we have looked at statistical models of the form $Y\sim Normal(X\beta,\sigma\mathbbm{1})$. This is a flexible framework, allowing us to model linear and non-linear relationships between a response and predictors. We covered model formulation, model fitting, model checking, model selection, hypothesis test on model fits, predictions from models and the design of experiments to effectively collect data for statistical analysis. Today we will look at an even more general class of statistical models than linear models.

\subsection{Where linear models are not enough}

\begin{example}
	Consider yes/no outcomes or count data:
	
	\begin{center}
		\begin{tikzpicture}[scale=0.8]
			\begin{axis}[
			xmin=2000, xmax=4500, xlabel=weight (kg),
			ymin=0, ymax=1, ylabel=proportion of cars that fail,
			title=\textbf{cars failing mileage tests},
			axis x line=bottom,
			axis y line=left,
			]
			\addplot[blue, only marks, mark=x] coordinates {
				(2100.00,0.02)
				(2300.00,0.05)
				(2500.00,0.00)
				(2700.00,0.09)
				(2900.00,0.26)
				(3100.00,0.38)
				(3300.00,0.61)
				(3500.00,0.74)
				(3700.00,0.90)
				(3900.00,0.94)
				(4100.00,1.00)
				(4300.00,1.00)
			};
			\end{axis}
		\end{tikzpicture}
		\begin{tikzpicture}[scale=0.8]
		\begin{axis}[
		xmin=0, xmax=50000, xlabel=months in service,
		ymin=0, ymax=60, ylabel=number of damage incidents,
		title=\textbf{damage in different ship types},
		axis x line=bottom,
		axis y line=left,
		]
		\addplot[blue, only marks, mark=x] coordinates {
			(127.00,0.00)
			(63.00,0.00)
			(1095.00,3.00)
			(1095.00,4.00)
			(1512.00,6.00)
			(3353.00,18.00)
			(2244.00,11.00)
		};
		\addlegendentry {A};
		\addplot[red, only marks, mark=x] coordinates {
			(44882.00,39.00)
			(17176.00,29.00)
			(28609.00,58.00)
			(20370.00,53.00)
			(7064.00,12.00)
			(13099.00,44.00)
			(7117.00,18.00)
		};
		\addlegendentry {B};
		\addplot[green, only marks, mark=x] coordinates {
			(1179.00,1.00)
			(552.00,1.00)
			(781.00,0.00)
			(676.00,1.00)
			(783.00,6.00)
			(1948.00,2.00)
			(274.00,1.00)
		};
		\addlegendentry {C};
		\addplot[yellow, only marks, mark=x] coordinates {
			(251.00,0.00)
			(105.00,0.00)
			(288.00,0.00)
			(192.00,0.00)
			(349.00,2.00)
			(1208.00,11.00)
			(2051.00,4.00)
		};
		\addlegendentry {D};
		\addplot[cyan, only marks, mark=x] coordinates {
			(45.00,0.00)
			(789.00,7.00)
			(437.00,7.00)
			(1157.00,5.00)
			(2161.00,12.00)
			(542.00,1.00)
		};
		\addlegendentry {E};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	
	The normal distribution for the errors $\epsilon$ is not appropriate here.
\end{example}

There is a more general class of models than linear models, called \begriff{Generalised Linear Models} (GLMs). They can be written as
\begin{align}
	\mathbb{E}(Y_i) &\equiv \mu_i =\gamma(X_i\beta) \notag \\
	Y_i &\overset{\text{independent}}{\sim} \text{ Exponential family distribution} \notag
\end{align}
where $\gamma$ is any smooth monotonic function. The Exponential family of distributions includes distributions such as Poisson, Gaussian, binomial and gamma.

GLMs are written in terms of the \begriff{link function}, $g$, which is the inverse of $\gamma$:
\begin{align}
	g(\mu_i) &= X_i\beta \notag \\
	Y_i &\overset{\text{independent}}{\sim} \text{ Exponential family distribution} \notag
\end{align}

\begin{example}
	\begin{align}
		Y_i &\sim \text{Binomial} \notag \\
		g(\mu) &= \ln\left(\frac{\mu}{1-\mu}\right)\notag
	\end{align}
	logit link function.
\end{example}

\begin{example}
	Linear models are a special case of GLMs.
\end{example}

The link functions in the following table are only examples. Other link functions can be used with distributions.
\begin{center}
	\begin{tabular}{c|c|c|c|c}
		\textbf{distribution} & \textbf{support} & \text{use} & \textbf{link name} & \textbf{link function} \\
		\hline
		normal & $(-\infty,\infty)$ & linear response & identity & $\mu$ \\
		exponential & $(0,\infty)$ & exponential response & inverse & $\mu^{-1}$ \\
		gamma & $(0,\infty)$ & gamma response & log & $\ln(\mu)$ \\
		Poisson & $\natur_0$ & count data & log & $\ln(\mu)$ \\
		binomial & & proportions of yes/no occurrences & logit & $\ln\left(\frac{\mu}{1-\mu}\right)$
	\end{tabular}
\end{center}

\begin{example}
	AIDS cases per year in Belgium at the start of the epidemic
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
			xmin=0, xmax=13, xlabel=year since 1980,
			ymin=0, ymax=250, ylabel=new AIDS cases,
			axis x line=bottom,
			axis y line=left,
			]
			\addplot[blue, only marks, mark=x] coordinates {
				(1,10)
				(2,15)
				(3,30)
				(4,55)
				(5,65)
				(6,70)
				(7,130)
				(8,150)
				(9,170)
				(10,210)
				(11,250)
				(12,245)
				(13,240)
			};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	Early in an epidemic, an exponential increase in cases can occur
	\begin{align}
		\mathbb{E}(Y_i) &\equiv \mu_i = \delta\exp(\alpha t_i) \notag \\
		Y_i &\sim Poisson(\mu_i) \notag
	\end{align}
	Taking the logarithm of both sides and letting $\beta_0\equiv\log(\delta)$ and $\beta_1\equiv\alpha$
	\begin{align}
		\log(\mu_i) &= \beta_0 + \beta_1 t_i \notag \\
		Y_i &\sim Poisson(\mu_i) \notag
	\end{align}
	which is a GLM with a log link.
\end{example}

\subsection{Model fitting in GLMs}

To fit GLMs to data, we use the principle of Maximum Likelihood Estimation (MLE). Given parameters $\beta$, we can write down $f(Y\mid \beta)$, the probability or probability density function of the response $Y$. For observed data, $Y_i^{\text{obs}}$, the likelihood function is
\begin{align}
	L(\beta) = \prod_i f(Y_i^{\text{obs}}\mid \beta)\notag
\end{align}
For GLMs, there is no closed form solution for the values of $\beta$ that maximise this function. Instead, MLE is performed numerically, using a technique called \begriff{iteratively re-weighted least squares} (IRLS). In principle, other optimisation algorithms could also be used for MLE.

\begin{example}[logistic regression]
	The proportion of cars of various weights that fail a mileage test.
	
	\begin{center}
		\begin{tikzpicture}
		\begin{axis}[
		xmin=2000, xmax=4500, xlabel=weight (kg),
		ymin=0, ymax=1, ylabel=proportion of cars that fail,
		title=\textbf{cars failing mileage tests},
		axis x line=bottom,
		axis y line=left,
		]
		\addplot[blue, only marks, mark=x] coordinates {
			(2100.00,0.02)
			(2300.00,0.05)
			(2500.00,0.00)
			(2700.00,0.09)
			(2900.00,0.26)
			(3100.00,0.38)
			(3300.00,0.61)
			(3500.00,0.74)
			(3700.00,0.90)
			(3900.00,0.94)
			(4100.00,1.00)
			(4300.00,1.00)
		};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	
	Data are bounded, so LM is not appropriate and we fit a GLM with
	\begin{align}
		Y_i &\sim Binomial(\mu_i) \notag \\
		g(\mu_i) &= X_i\beta = \beta_0 + \beta_1\cdot weight_i \notag
	\end{align}
	Here $g(\mu_i) = \ln\left(\frac{\mu_i}{1-\mu_i}\right)$, so $\mu_i=\frac{1}{1+\exp(-X_i\beta)}$, the logistic function.
\end{example}

\subsection{GLM assumptions and checking}

Important GLM assumptions:
\begin{itemize}
	\item Independence
	\item Distributional assumptions
	\item Weak exogeneity
	\item Linear relationship between transformed response and predictors (link function)
\end{itemize}
Residual plots are still useful to check if model assumptions hold.

As we use different distributions, we cannot simply use raw residuals, as for linear models. Two common types of residuals that attempt to mimic behaviour of residuals for LMs:
\begin{itemize}
	\item \person{Pearson} Residuals
	\item Deviance Residuals
\end{itemize}

Residual plots using deviance residuals.

\input{./TeX_files/materials/perfect_residuals_1_GLM}
\input{./TeX_files/materials/perfect_residuals_2_GLM}

Measures like Leverage and \person{Cook}'s distance can be used to look for outliers in the data.

\textbf{Warning:} MATLAB uses raw residuals as default.

\subsection{Hypothesis tests on GLM fits}

As for LMs, hypothesis tests for individual parameters have been developed. They test the hypothesis that $Y$ does not change as an explanatory changes. In other words, we test hypotheses like: \\
$H_0$: $\beta_1=0$ \\
$H_A$: $\beta_1\neq 0$ \\
... we skip the details of these tests (different software may using different tests).

As discussed previously, we could also use Likelihood-ratio tests to look at similar hypotheses. For global tests on the entire model, the Likelihood-ratio test can be used. E.g. for a model with $p$ parameters, compare the fitted model to the constant model by testing: \\
$H_0$: $\beta_1=\dots=\beta_{p-1}=0$

\subsection{Model selection on GLMs}

Model selection for GLMs proceeds in a similar way to what we discussed for LMs. However, some tests that were developed specifically for LMs are not usually appropriate.

AIC and BIC can always be used.

To compare nested models, the Likelihood-ratio test can be  used (versions of the F-test can only be used with great caution). Parameter-specific tests are available (although the likelihood-ratio test could be used for this). A similar measure to $R^2$ exists (\begriff{Deviance}). It is based on comparing the likelihood of the model to a \begriff{saturated model} with one parameter per data point.

\subsection{Interpreting GLM parameters}

From our model formulation and fit, we find that
\begin{align}
	\mu_i = \frac{1}{1+\exp(-(-13.38+0.0042\cdot weight_i))} \notag
\end{align}

\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	xmin=2000, xmax=4500, xlabel=weight (kg),
	ymin=0, ymax=1, ylabel=proportion of cars that fail,
	title=\textbf{cars failing mileage tests},
	axis x line=bottom,
	axis y line=left,
	domain=2000:4500,
	]
	\addplot[blue, only marks, mark=x] coordinates {
		(2100.00,0.02)
		(2300.00,0.05)
		(2500.00,0.00)
		(2700.00,0.09)
		(2900.00,0.26)
		(3100.00,0.38)
		(3300.00,0.61)
		(3500.00,0.74)
		(3700.00,0.90)
		(3900.00,0.94)
		(4100.00,1.00)
		(4300.00,1.00)
	};
	\addplot[cyan] {1/(1+exp(-(-13.38 + 0.0042*x)))};
	\end{axis}
	\end{tikzpicture}
\end{center}

Unlike for linear models, we cannot read off effect sizes directly from parameter estimates in GLMs. \textcolor{red}{Need to consider the link function.}