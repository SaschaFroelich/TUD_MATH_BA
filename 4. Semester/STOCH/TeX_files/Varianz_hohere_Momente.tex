\section{Varianz und höhere Momente}
\begin{definition}[$k$-te Momente]
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, $X: (\O,\F) \to (\R, \borel(\R))$ reelle Zufallsvariable. Dann ist für $k \in \N$
	\begin{align*}
		\E[X^k] = \int_{\O} X^k(\omega)\P(\omega) = \int_{\R}x^k \P(X \in \d x)
	\end{align*}
	das \begriff{$k$-te Moment} von $X$ (sofern definiert).
\end{definition}
\begin{*remark}
	\begin{itemize}
		\item Erwartungswert $\cong$ erstes Moment
		\item Das $k$-te Moment existiert genau dann wenn
		\begin{align*}
			\int_{\O} \abs{X(\omega)^k}\P(\d \omega) < \infty \bzw X \in \Ln{k}(\P)
		\end{align*}
		\item Mint: $\Ln{r}(\P) \subseteq \Ln{s}(\P)$ für $s \le r$
	\end{itemize}
\end{*remark}
Von Interesse ist insbesondere das zweite Moment.
\begin{definition}[Varianz, Standardabweichung]
	\proplbl{5_2_11}
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, $X,Y \in \Ln{2}(\P)$ reelle Zufallsvariablen.
	\begin{enumerate}
		\item Die \begriff{Varianz} von $X$ ist
		\begin{align*}
			\Var (X) = \E[(X - \E[X])^2] = \E[X^2] - (\E[X])^2
		\end{align*}
		\item Die \begriff{Standardabweichung}/\begriff{Streuung} von $X$ ist $\sqrt{\Var X}$.
		\item Die \begriff{Kavarianz} von $X$ und $Y$ ist
		\begin{align*}
			\Cov(X,Y) &= \E[(X-\E[X])(Y-\E[Y])]\\
			&= \E[XY] - \E[X]\E[Y]
		\end{align*}
		\begin{*hint}
			Wenn die Varianz 0 ist, heißt es nicht dass die Zufallsvariablen unabhängig waren
		\end{*hint}
		\item Sind $\Var X, \Var Y\ge 0$, dann ist die \begriff{Korrelation} von $X \und Y$
		\[
			\Corr(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.
		\] 
		\item Gilt $\Corr(X,Y) =0$, so heißen $X,Y$ \begriff{unkorreliert}.
	\end{enumerate}
\end{definition}
\begin{*remark}
	\begin{itemize}
		\item Die Endlichkeit der Ausdrücke in \propref{5_2_11} folgt aus der \person{Cauchy}-\person{Schwarz}-Ungleichung
		\[
		\E[\abs{XY}] \le (\E\abs{X}^2)^{\frac{1}{2}}\cdot (\E\abs{Y}^2)^{\frac{1}{2}}
		\]
		\item Für die (Ko)varianz gilt
		\begin{align*}
			\E[(X -\E[X])(Y-\E[Y])] &= \E[XY - X\E[Y] - Y\E[X] + \E[X]\E[Y]]\\
			&= \E[XY] - \E[X]\E[Y] - \E[X]\E[Y] + \E[X]\E[Y]\\
			&= \E[XY] - \E[X]\E[Y].
		\end{align*}
	\end{itemize}
\end{*remark}
\begin{example}
	Sei $X\sim\Bin(n,p)$, dann gilt
	\begin{align*}
		\Var(X) = \E X^2 - \underbrace{(\E X)^2}_{=\sim p}\\
		\intertext{mit}
		\E X^2 &= \sum_{k=0}^n k^2 \binom{n}{k}p^k (1-p)^{n-k}\\
		&= np \sum_{i=1}^n k \frac{(n-1)!}{(n-1-(k-1))!(k-1)!} p^{k-1}(1-p)^{n-1-(k-1)}\\
		&= np\sum_{l=0}^{n-1}(l+1)\binom{n-1}{l}p^l (1-p)^{n-1-l}\\
		&= np(1 + \sum_{l=0}^{n-1}l \binom{n-1}{l}p^l (1-p)^{n-1-l})\\
		&= np(1+(n-1)p)\cdot 1)\\
		&= np + n(n-1)p^2\\
		&\implies \Var X = np + n^2p^2 - np^2 - n^2p^2\\
		&= np(1-p). 
	\end{align*}
\end{example}
\begin{proposition}[Eigenschaften der (Ko-)varianz]
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, $X,Y, X_1, \dots, X_n \in \Ln{2}(\P), a,b \in \R$.
	\begin{enumerate}
		\item $\Var(aX + b) = a^2\Var(X)$
		\item  Sei
		\begin{align*}
			(\Cov(X,Y))^2 &\le \Var X \Var Y
			\intertext{und insbesondere}
			\abs{\Corr(X,Y)} &\le 1
		\end{align*}
		\item $\Var\brackets{\sum_{i=1}^n X_i} = \sum_{i=1}^n \Var(X_i) + \sum_{i=1}^n \Var(X_i,X_j)$ Sind die $X_1, \dots, X_n$ paarweise unkorreliert, so gilt die \begriff{Formel von \person{Bienaymé}}: %TODO how is he called?
		\[
			\Var\brackets{\sum_{i=1}^n X_i} = \sum_{i=1}^n \Var(X_i)
		\]
		\item $X \upmodels Y \implies \Corr(X,Y) = 0$
		\item \begriff{\person{Tschebyscheff}-Ungleichung}: Für $\epsilon > 0$
		\[
			\P(\abs{X - \E X} > \epsilon) \le \frac{\Var X}{\epsilon^2}
		\]
	\end{enumerate}
\end{proposition}
\begin{proof} %TODO add references!
	\begin{enumerate}
		\item Da $\E[aX + b] = a\E X + b$, folgt 
		\begin{align*}
			\Var(aX+b) &= \E[(a\E X +b - (a\E X + b))^2]\\
			&= \E[a^2(X-\E X)^2] = a^2 \Var X.
		\end{align*}
		\item Wegen 1. können wir oBdA annehmen, dass $\E X = 0 = \E Y$. Dann wird 2. zu
		\begin{align*}
			\E[XY]^2 \le \E X^2 \cdot \E Y^2
		\end{align*}
		und dies ist die \person{Cauchy}-\person{Schwarz} Ungleichung.
		\item Wähle wieder oBdA $\E X_i = 0$. Dann 
		\begin{align*}
			\Var \brackets{\sum_{i=1}^n X_i} &= \E \sqbrackets{(\sum_{i=1}^n X_i)^2}\\
			&= \E \sqbrackets{\sum_{i,j = 1}^n X_i X_j} \\
			&= \sum_{i,j = 1}^n \E [X_i X_j]\\
			&= \sum_{i,j = 1}^n \Cov(X_i, X_j) \\
			&= \sum_{i=1}^n \Var(X_i) + \sum_{\substack{i,j = 1\\ i \neq j}}^n \Cov(X_i, X_j)
		\end{align*} 
		\item Sei
		\begin{align*}
			X \upmodels Y &\implies \E[XY]=\E X \E Y\\
			&\implies \Cov(X,Y) = \E[XY] - \E X \E Y = 0\\
			&= \Corr(X,Y) = 0
		\end{align*}
		\item Wende die \person{Markov}-Ungleichung auf $X' = (X-\E X)^2$ an, dann folgt
		\begin{align*}
			\P(\abs{X - \E X} > \epsilon) &\le \P((X - \E X)^2 > \epsilon^2)\\
			\over{\person{Markov}}&{\le} \frac{1}{\epsilon^2} \E [(X - \E X)^2]\\
			&= \frac{\Var X}{\epsilon^2}.
		\end{align*}
	\end{enumerate}
\end{proof}