\chapter[Bedingte Wahrscheinlichkeiten und (Un)-abbhängigkeit]{Bedingte Wkeiten und (Un)-abbhängigkeit}
\chaptermark{Bedingte Wheiten und (Un)-abbhängigkeit}

\section{Bedingte Wahrscheinlichkeiten}
\begin{example}
	\proplbl{3_1_1}
	Das Würfeln mit zwei fairen, sechsseitigen Würfeln können wir mit 
	\begin{align}
		\Omega = \set{(i,j,), i,j \in \set{1,\dots,6}}\notag
	\end{align}
	und $\probp = \Gleich(\Omega)$. Da $\abs{\Omega} = 36$ gilt also
	\begin{align}
		\probp(\set{\omega}) = \frac{1}{36} \quad \forall \omega \in \Omega.\notag
	\end{align}
	Betrachte das Ereignis
	\begin{align}
		A = \set{(i,j) \in \Omega : i + j = 8},\notag
	\end{align}
	dann folgt
	\begin{align}
		\probp(A) = \frac{5}{36}.\notag
	\end{align}
	Werden die beiden Würfel nach einander ausgeführt, so kann nach dem ersten Wurf eine Neubewertung der Wahrscheinlichkeit von $A$ erfolgen.\\
	Ist z.B.:
	\begin{align}
		B = \set{(i,j) \in \Omega, i = 4}\notag
	\end{align}
	eingetreten, so kann die Summe 8 nur durch eine weitere 4 realisiert werden, also mit Wahrscheinlichkeit
	\begin{align}
		\frac{1}{6} = \frac{\abs{A \cap B}}{\abs{B}}.\notag 
	\end{align}
	Das Eintreten von $B$ führt also dazu, dass das Wahrscheinlichkeitsmaß $\probp$ durch ein neues Wahrscheinlichkeitsmaß $\probp_{B}$ ersetzt werden muss. Hierbei sollte gelten:
	\begin{align}
		 &\text{Renormierung: }\probp_{B} = 1\label{Renorm}\tag{R}\\
		 &\text{Proportionalität: Für alle} A \subset \sigF \mit A \subseteq B \text{ gilt }
		 \probp_{B}(A) = c_B \probp(A) \text{ mit einer Konstante } c_B.\label{Prop}\tag{P}
    \end{align}
\end{example}

\begin{lemma}
	Sei $(\Omega, \sigF, \probp)$ Wahrscheinlichkeitsraum und $B \in \sigF$ mit $\probp(B) > 0$. Dann gibt es genau ein Wahrscheinlichkeitsmaß $\probp_B$ auf $(\Omega, \sigF)$ mit den Eigenschaften \eqref{Renorm} und \eqref{Prop}. Dieses ist gegeben durch
	\begin{align}
		\probp_{B}(A) = \frac{\probp(A\cap B)}{\probp(B)} \quad \forall A \in \sigF.\notag
	\end{align}
\end{lemma}

\begin{proof}
	Offenbar erfüllt $\probp_{B}$ wie definiert \eqref{Renorm} und \eqref{Prop}. Umgekehrt erfüllt $\probp_{B}$ \eqref{Renorm} und \eqref{Prop}. Dann folgt für $A \in \sigF$:
	\begin{align}
		\probp_{B}(A) = \probp_{B}(A\cap B) + \underbrace{\probp_{B}(A\setminus B)}_{= 0, \text{ wegen } \eqref{Renorm}} \overset{\eqref{Prop}}{=} c_B \probp(A \cap B).\notag
	\end{align}
	Für $A=B$ folgt zudem aus \eqref{Renorm}
	\begin{align}
		1 = \probp_{B}(B) = c_B \probp(B)\notag
	\end{align}
	also $c_B = \probp(B)^{-1}$.
\end{proof}

% % % % % % % % % % % % % % % % % % % % % % % % % % % 5th lecture % % % % % % % % % % % % % % % % % % % % % % % % % % %

\begin{definition}
	\proplbl{3_1_3}
	Sei $(\Omega, \sigF, \probp)$ Wahrscheinlichkeitsraum und $B \in \sigF$ mit $\probp(B) > 0$. Dann heißt
	\begin{align*}
		\probp(A\vert B) := \frac{\probp(A\cap B)}{\probp(B)} \mit A\in \sigF
	\end{align*}
	die \begriff{bedingte Wheit von $A$ gegeben $B$}.
	Falls $\probp(B) = 0$, setze
	\begin{align*}
		\probp(A \vert B) = 0 \mit \forall A \in \sigF
	\end{align*}
\end{definition}

\begin{example} %TODO ref
	In der Situation \propref{3_1_1} gilt % 
	\begin{align*}
	A \cap B = \set{(4,4)}
	\intertext{und damit}
	\probp(A \vert B) = \frac{\probp(A\cap B)}{\probp(B)} = \frac{\frac{1}{36}}{\frac{1}{6}} = \frac{1}{6}
	\end{align*}
\end{example}
Aus \propref{3_1_3} ergibt sich
\begin{lemma}[Multiplikationsformel]
	\proplbl{3_1_4}
	Sei $(\Omega, \sigF, \probp)$ Wahrscheinlichkeitsraum und $A_1, \dots, A_n \in \sigF$. Dann
	\begin{align*}
		\probp(A_1 \cap \cdots \cap A_n) = \probp(A_1)\probp(A_2 \vert A_n) \dots \probp(A_n \vert A_1 \cap \cdots \cap A_{n-1})
	\end{align*}
\end{lemma}

\begin{proof}
	Ist $\probp(A_1 \cap \dots \cap A_n) = 0$, so gilt auch $\probp(A_n \vert \bigcap_{i=1}^{n-1}) = 0$. Andernfalls sind alle Faktoren der rechten Seite ungleich 0 und
	\begin{align*}
		\probp(A_1)\probp(A_2 \vert A_1) \dots \probp(A_n \vert \bigcap_{i=1}^{n-1} A_i) \\
		= \probp(A_1) \cdot \frac{\probp(A_1 \cap A_2)}{\probp(A_1)} \dots \frac{\probp(\bigcap_{i=1}^{n} A_i)}{\probp(\bigcap_{i=1}^{n-1}A_i)} = \probp(\bigcap_{i=1}^n A_i)	
	\end{align*}
\end{proof} %TODO add ref.
Stehen die $A_i$ in \propref{3_1_4} in einer (zeitlichen) Abfolge, so liefert Formel einen Hinweis, wie Wahrscheinlichkeitsmaße für \begriff{Stufenexperimente} konstruiert werden können. Ein \emph{Stufenexperiment} aus $n$ nacheinander ausgeführten Teilexperimenten lässt sich als \begriff{Baumdiagramm} darstellen.

%TODO Baumdiagramm
%\begin{tikzpicture}
%	%TODO
%\end{tikzpicture}

\begin{proposition}[Konstruktion des Wahrscheinlichkeitsmaßes eines Stufenexperiments]
	\proplbl{3_1_6}
	Gegeben seinen $n$ Ergebnisräume $\Omega_i = \set{\omega_i (1), \dots, \omega_i (k)}, k \in \N \cup \set{\infty}$ und es sei $\Omega = \bigtimes_{i = 1}^n \Omega_i$ der zugehörige Produktraum. Weiter seinen $\sigF_i$ $\sigma$-Algebren auf $\Omega_i$ und $\sigF = \bigotimes_{i=1}^n \sigF_i$ die Produkt-$\sigma$-Algebra auf $\Omega$. Setze $\omega = (\omega_1,\dots,\omega_n)$ und
	\begin{align*}
		[\omega_1,\dots,\omega_n]:= \set{\omega_1}\times \dots \times \set{\omega_n} \times \Omega_{m-1} \times \Omega_{n},\quad m\le n\\
		\probp(\set{\omega_m}[\omega_1,\dots,\omega_{m-1}])
	\end{align*} %TODO check indices, m-1 instead of m+1?
	für die Wheit in der $m$-ten Stufe des Experiments $\omega_m$ zu beobachten, falls in den vorausgehenden Stufen $\omega_1,\dots,\omega_{m-1}$ beobachten wurden. Dann definiert
	\begin{align*}
		\probp(\set{\omega}) := \probp(\set{\omega_1})\prod_{m=2}^{n}\probp\brackets{\set{\omega_m} \mid [\omega_1, \dots, \omega_{m-1}]}
		%TODO maybe wrong here. check
	\end{align*}
	ein Wahrscheinlichkeitsmaß auf $(\Omega, \sigF, \probp)$.
\end{proposition}

\begin{example}[\person{Polya}-Urne]
	Gegeben sei eine Urne mit $s$ Schwarze und $w$ weiße Kugeln. Bei jedem Zug wird die  gezogene Kugel zusammen mit $c\in \N_0\cup \set{-1}$ weiteren Kugeln derselben Farbe zurückgelegt.
	\begin{itemize} %TODO seen both in chapter 2.2, but big bracket behind.
		\item $c=0$: Urnenmodell mit Zurücklegen
		\item $c=-1$: Urnenmodell ohne Zurücklegen
	\end{itemize}
	Beide schon in Kapitel 2.2 gesehen.\\
	Sei $c\in \N$. (Modell für zwei konkurrierende Populationen) Ziehen wir $n$-mal, so erhalten wir ein $n$-Stufenexperiment mit 
	\begin{align*}
		\Omega = \set{0,1}^n \mit \text{ 0 = ``weiß'', 1 = ``schwarz''}\mit	(\Omega_i = \set{0,1})
		\intertext{Zudem gelten im ersten Schritt}
		\probp(\set{0}) = \frac{w}{s+w} \und \probp(\set{1}) = \frac{s}{w+s}
		\intertext{sowie}
		\probp(\set{\omega_m} \mid [\omega_1, \dots \omega_{m-1}]) = 
		\begin{cases} %TODO fix brackets!
		\frac{w+c(m-1 - \sum_{i=1}^{m-1}\omega_i)}{s+w+c(m-1)} & \omega_m = 0\\
		\frac{s + c\sum_{i=1}^{m-1}\omega_i}{s+w+c(m-1)} & \omega_m = 1
		\end{cases}
	\end{align*}
	Mit \propref{3_1_6} folgt als Wahrscheinlichkeitsmaß auf $(\Omega, \pows(\Omega))$
	\begin{align*}
		\probp(\set{(\omega_1, \dots, \omega_n)}) &= \probp(\set{\omega_1})\prod_{m=2}^n \probp(\set{\omega_m}\mid [\omega_1,\dots,\omega_{m-1}])\\
		&=\frac{\prod_{i=0}^{l-1}(s+c_i)\prod_{i=0}^{n-l-1}}{\prod_{i=0}^n (s+w+c_i)} \mit l=\sum_{i=1}^n \omega_i.
		\intertext{Definiere wir nun die Zufallsvariable}
		S_n:\Omega &\to \N_0 \mit (\omega_1, \dots, \omega_n) \mapsto \sum_{i=1}^n \omega_i
		\intertext{welche die Anzahl der gezogenen schwarzen Kugeln modelliert, so folgt,}
		\probp(S_n = l) &= \binom{n}{l}\frac{\prod_{i=0}^{l-1}(s+c_i) \prod_{j=0}^{n-l-1}(\omega + c_j)}{\prod_{i=0}^n(s+w+c_i)}
		\intertext{Mittels $a:= \sfrac{s}{c},b:= \sfrac{w}{c}$ folgt}
		\probp(S_n = l) &= \frac{\prod_{i=0}^{l-1}(-a-i)\prod_{i=0}^{-b-j-1}}{\prod_{i=0}^n (-a-b-i)} = \frac{\binom{-a}{l}\binom{-b}{n-l}}{\binom{-a-b}{n}}\\ &\mit l \in \set{0,\dots,n} 
	\end{align*}
	Dies ist die \begriff{\person{Polya}-Urne} auf $\set{0,\dots,n}, n \in \N$ mit Parametern $a,b > 0$.
\end{example}

\begin{example}
	Ein Student beantwortet eine Multiple-Choice-Frage mit 4 Antwortmöglichkeiten, eine davon ist richtig. Er kennt die richtige Antwort mit Wheit $\sfrac{1}{3}$. Wenn er diese kennt, so wählt er diese aus. Andernfalls wählt er zufällig (gleichverteilt) eine Antwort.\\
	Betrachte
	\begin{align*}
		W = \set{\text{richtige Antwort gewusst}}\\
		R = \set{\text{Richtige Antwort gewählt}}
		\intertext{Dann}
		\probp(W) = \frac{2}{3}, \probp(R \vert W) = 1, \probp(R \vert W^C) = \frac{1}{4} 
	\end{align*}
	Angenommen, der Student gibt die richtige Antwort. Mit welcher Wahrscheinlichkeit hat er diese gewusst?
	\begin{align*}
		\probp(W\vert R) = \text{ ?}
	\end{align*}
\end{example}

\begin{proposition}
	\proplbl{3_1_9}
	Sei $(\Omega, \sigF, \probp)$ Wahrscheinlichkeitsraum und $\Omega = \bigcup_{i \in I} B_i$ eine höchstens abzählbare Zerlegung in paarweise disjunkte Ereignisse $B_i \in \sigF$.
	\begin{enumerate} %TODO set itemize references. or use enumerate?
		\item \emph{Satz von der totalen Wahrscheinlichkeit:} Für alle $A \in \sigF$
		\begin{align*}
			\probp(A) = \sum_{i\in I} \probp(A\vert B_i)\probp(B_i) \label{eq:totWkeit}\tag{totale Wahrscheinlichkeit}
		\end{align*} 
		\item \emph{Satz von \person{Bayes}:} Für alle $A \in \sigF$ mit $\probp(A) > 0$ und alle $h \in I$
		\begin{align*}
			\probp(B_h \vert A) = \frac{\probp(A \vert B_h)\probp(B_h)}{\sum_{i\in I}\probp(A\vert B_i)\probp(B_i)} \label{eq:bayes}\tag{Bayes}
		\end{align*}
	\end{enumerate}
\end{proposition}

\begin{proof}
	\begin{enumerate}
		\item Es gilt:
		\begin{align*}
			\sum_{i\in I} \probp(A\vert B_i)\probp(B_i) \defeq \sum_{i\in I}\frac{\probp(A \cap B_i)}{\probp(B_i)}\probp(B_i) = \sum_{i\in I} \probp(A \cap B_i) \overset{\sigma-add.}{=} \probp(A)
		\end{align*}
		\item 
		\begin{align*}
			\probp(B_h \vert A) \defeq \frac{\probp(A \cap B_h)}{\probp(A)} \defeq \frac{\probp(A \vert B_h)\probp(B_h)}{\probp(A)}
		\end{align*}
		also mit a) auch b). %TODO add refs
	\end{enumerate}
\end{proof}

\begin{example}
	In der Situation von \propref{3_1_3} folgt mit dem \propref{3_1_9} \eqref{eq:totWkeit} der totalen Wahrscheinlichkeit
	\begin{align*}
		\probp(R) &= \probp(R \vert W)\probp(W) + \probp(R\vert W^C)\probp(W^C)\\
		&= 1\cdot \frac{2}{3} + \frac{1}{4}\frac{1}{3} = \frac{3}{4}
		\intertext{und mit dem \propref{3_1_9} \eqref{eq:bayes}} %Bayes
		\probp(W \vert R) &= \frac{\probp(R \vert W)\probp(W)}{\probp(R)} = \frac{1\frac{2}{3}}{\frac{3}{4}} = \frac{8}{9} \text{ für die gesuchte Wahrscheinlichkeit.}
	\end{align*} %TODO
%	\begin{tikzpicture}
%		
%	\end{tikzpicture} 
\end{example}
% trees in Latex?
% https://tex.stackexchange.com/questions/5447/how-can-i-draw-simple-trees-in-latex/5451
\section{(Un)-abhängigkeit} \label{sec_unabhangigkeit}
In vielen Fällen besagt die Intuition über verschiedene Zufallsexperimente/ Ereignisse, dass diese sich \emph{nicht} gegenseitig beeinflussen. Für solche $A,B \in \F$ mit $\probp(A) > 0, \probp(B) > 0$ sollte gelten
\begin{align*}
	\probp(A\mid B) = \probp(B), \quad \probp(B\mid A) = \probp(B).
\end{align*}
\begin{definition}[(stochastisch)unabhängig]
	\proplbl{3_2_11}
	$(\Omega, \F, \probp)$ WRaum. Zwei Ereignisse $A,B \in \F$ heißt \begriff{(stochastisch) unabhängig bezüglich $\probp$}, falls
	\begin{align*}
		\probp(A\cap B) = \probp(A)\probp(B)
	\end{align*}
	Wir schreiben auch $A B$.
\end{definition}
\begin{example}
	Würfeln mit 2 fairen, sechsseitigen Würfel:
	\begin{align*}
		\Omega = \set{(i,j) \mid i,j \in\set{1,\dots,n}},\quad \F = \pows(\Omega), \quad \probp = \Gleich(\Omega)
	\intertext{Betrachte}
		A:= \set{(i,j) \in \Omega, i \text{ gerade}}\\
		B:= \set{(i,j) \in \Omega, j > 2}
	\end{align*}
	In diesem Fall, erwarten wir intuitiv Unabhängigkeit von $A$ und $B$.\\
	In der Tat % start using \P instead of \probp!
	\begin{align*}
		\P(A) = \frac{1}{2}, \quad \P(B) = \frac{1}{3} \mit \P(A\cap B) = \frac{1}{6}
		\intertext{erfüllt}
		\P(A\cap B) = \P(A)\P(B).
	\end{align*}
	Betrachte nun
	\begin{align*}
		C:= \set{(i,j) \in \Omega, i\neq j = 1}\\
		D:= \set{(i,j) \in \Omega, i = 6}
		\intertext{dann gilt}
		\P(C) = \frac{1}{6}, \quad \P(D) = \frac{1}{6}
		\intertext{und wegen $C \cap D = \set{(6,1)}$ folgt}
		\P(C\cap D) = \frac{1}{36} = \frac{1}{6} \frac{1}{6} = \P(C \setminus D)
	\end{align*}
	$C$ und $D$ sind also \emph{stochastisch} unabhängig, obwohl eine kausale Abhängigkeit vorliegt!
\end{example}
\begin{definition}[unabhängig bezüglich $\P$]
	$(\Omega, \F, \P)$ WRaum und $I \neq \emptyset$ endliche Indexmenge. Dann heißt die Familie $(A_i)_{i \in I}$ von Ereignissen in $\F$ \begriff{unabhängig bezüglich $\P$}, falls für alle $J \subseteq I, J \neq \emptyset$ gilt:
	\begin{align*}
		\P\brackets{\bigcap_{i\in J}A_i} = \prod_{i\in J} \P(A_i)
	\end{align*}
	Offensichtlich impliziert die Unabhängigkeit einer Familie die paarweise Unabhängigkeit je zweier Familienmitglieder nach \propref{3_2_11}. Umgekehrt gilt dies nicht!
\end{definition}
\begin{example}[Abhängigkeit trotz paarweiser Unabhängigkeit]
	Betrachte 2-faches Bernoulliexperiment mit Erfolgswahrscheinlichkeit $\sfrac{1}{2}$, d.h.
	\begin{align*}
		\Omega = \set{0,1}^2, \quad \F = \pows(\Omega), \quad \P = \Gleich(\Omega)
		\intertext{sowie}
		A = \set{1}\times \set{0,1} \qquad \text{(Münzwurf: erster Wurf Zahl)}\\
		B = \set{0,1}\times \set{1} \qquad \text{(Münzwurf: zweiter Wurf Zahl)}\\
		C = \set{0,0}\times \set{1,1} \qquad \text{(beide Würfe selbes Ergebnis)}
	\end{align*}
	Dann gelten
	\begin{align*}
		\P(A) = \frac{1}{2} = \P(A) = \P(B)
		\intertext{und}
		\P(A\cap B) = \P(\set{(1,1)}) = \frac{1}{4} = \P(A)\P(B)\\
		\P(A\cap C) = \P(\set{(1,1)}) = \frac{1}{4} = \P(A)\P(C)\\
		\P(B\cap C) = \P(\set{(1,1)}) = \frac{1}{4} = \P(B)\P(C)
	\end{align*}
	also paarweise Unabhängigkeit.\\
	Aber
	\begin{align*}
		\P(A\cap B \cap C) = \P(\set{(1,1)}) = \frac{1}{4} + \P(A)\P(B)\P(C)
		\intertext{und $A,B,C$ sind \emph{nicht} stochastisch unabhängig.}
	\end{align*}
\end{example}
\begin{definition}[Unabhängige Messräume]
	\proplbl{3_2_15}
	% started using \O for \Omega and \E for this special generating set E_i
	$(\O, \F,\P)$ Wahrscheinlichkeitsraum, $I \neq \emptyset$ Indexmenge und $(E_i, \E_i)$ Messräume
	\begin{enumerate}
		\item Die Familie $\F_i \subset \F, i \in I$, heißen \emph{unabhängig}, wenn für die $J \subseteq I, I \neq \emptyset, \abs{J} < \infty$ gilt
		\begin{align*}
			\P\brackets{\bigcap_{i=1} A_i} = \prod_{i\in J} \P(A_i) \qquad \text{ für beliebige } A_i \in \F_i, i \in J
		\end{align*}
		\item Die Zufallsvariable $X_i: (\O, \F) \to (E_i, \E_i), i \in I$, heißen \emph{unabhängig}, wenn die $\sigma$-Algebren
		\begin{align*}
			\sigma(X_i) = X^{-1}(\E_i) = \set{\set{X_i \in F}, F \in \E_i}, \quad i \in I
		\end{align*}
		unabhängig sind.
	\end{enumerate}
\end{definition}
\begin{lemma}[Zusammenhang der Definitionen]
	\proplbl{3_2_16}
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, $I \neq \emptyset, A \in \F, i \in I$.\\
	Die folgenden Aussagen sind äquivalent:
	\begin{enumerate}
		\item Die Ereignisse $A_i, i \in I$ sind unabhängig. 
		\item Die $\sigma$-Algebren $\sigma(A_i), i \in I$ sind unabhängig.
		\item Die Zufallsvariablen $\indi_{A_i}, i \in I$ sind unabhängig.
	\end{enumerate}
\end{lemma}
\begin{proof} %TODO add ref?
	Da die Unabhängigkeit über endliche Teilemengen definiert ist, können wir oBdA $I = \set{1, \dots, n}$ annehmen. 
	\begin{itemize}
		\item Da $\sigma(\indi_{A_i}) = \sigma(A_i)$ folgt die Äquivalenz von 2. und 3. direkt aus \propref{3_2_15}.
		\item Zudem ist 2. $\to $ 1. klar!
		\item Für 1 $\to$ 2. genügt es zu zeigen, dass
			\begin{align*}
				A_1, \dots, A_n \text{ unabhängig } &\Rightarrow B_1, \dots, B_n \text{ unabhängig von } B_i \in \set{\emptyset, A_i, A_i^C, \O}.
				\intertext{Rekursive folgt dies bereits aus}
				A_1,\dots, A_n \text{ unabhängig } &\Rightarrow B_1, A_2, \dots, A_n \text{ unabhängig mit } B_1 \in \set{\emptyset, A_1, A_1^C, \O}.
			\end{align*}
		Für $B_1 \in \set{\emptyset, A_1, \O}$ ist dies klar.\\
		Sei also $B_1 = A_1^C$ und $J \subseteq I, J \neq \emptyset$. Falls $1 \not \in J$, ist nichts zu zeigen. Sei $1 \in J$, dann gilt mit
			\begin{align*}
				A &= \bigcap_{i\in J, i \neq 1} A_i
				\intertext{sicherlich}
				\P\brackets{A_1^C \cap A} &= \P(A \setminus (A_1 \cap A))\\
				&= \P(A) - \P(A_1 \cap A)\\
				&= \prod_{i\in J\setminus \set{1}} \P(A_i) - \prod_{i\in J}(A_i)\\
				&= (1- \P(A_1))\prod_{i\in J\setminus \set{1}} \P(A_i)\\
				&= \P\brackets{A_1^C})\prod_{i\in J\setminus \set{1}} \P(A_i)
			\end{align*} 
	\end{itemize}
\end{proof}
Insbesondere zeigt das \propref{3_2_16}, dass wir in einer Familie unabhängiger Ereignisse beliebig viele Ereignisse durch ihr Komplement, $\emptyset$ oder $\O$ ersetzen können, ohne die Unabhängigkeit zu verlieren.
\begin{proposition}
	\proplbl{3_2_17}
	$(\O, \F, \P)$ Wahrscheinlichkeitsraum und $\F_i \subseteq \F, i \in I$, seien $\cap$-stabil Familien von Ereignissen. Dann
	\begin{align*}
		\F_i, i \in I \text{ unabhängig } \Leftrightarrow \sigma(\F_i), i \in I \text{ unabhängig}. 
	\end{align*}
\end{proposition}

\begin{proof}
	OBdA sei $I = \set{1, \dots, n}$ und das $\O \in \F_i, i \in I$.
	\begin{itemize}
		\item $\Leftarrow$: trivial, da $\F_i \subseteq \sigma(\F_i)$ und das Weglassen von Mengen erlaubt ist.
		\item $\Rightarrow$: zeigen wir rekursive
			\begin{enumerate}
				\item Wähle $F_i \in \F_i, i = 2, \dots,n$ und defnieren für $F \in \sigma(\F_i)$ die endlichen Maße
				\begin{align*}
					\mu(F) = \P\brackets{\bigcap_{i=1}^n F_i} \und \nu(F) = \prod_{i=1}^n \P(F_i)
				\end{align*}
				\item Da die Familien $\F_i$ unabhängig sind, gilt
				\begin{align*}
					\mu\mid_{\F_1} = \nu\mid_{\F_1}
				\end{align*}
				Nach Eindeutigkeitssatz für Maße (\proplbl{1_1_19}) folgt $\mu\mid_{\sigma(\F_1)} = \nu\mid_{\sigma(\F_1)}$ also
				\begin{align*}
					\P(\bigcap_{i=1}^n F_i) = \P(F)\P(F_1)\dots \P(F_n)
				\end{align*}
				für alle $F \in \sigma(\F_i)$ und $F_i \in \F_i, i = 1, \dots, n$. Da $\O \in \F_i$ für alle $i$ gilt die erhaltene Produktformel auf für alle Teilemenge $J \subseteq I$.\\
				Also sind
				\begin{align*}
					\sigma(\F_1), \F_2, \dots, \F_n \text{unabhängig}
				\end{align*}
				\item Wiederholtes Anwenden von $1$ und $2$ liefert den \propref{3_2_17}.
			\end{enumerate}
	\end{itemize}
\end{proof}

Mittels \propref{3_2_17} folgen:

\begin{conclusion}
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum und
	\begin{align*}
		\F_{i,j} \subseteq \F, \quad 1 \le \dots \le n, 1 \le j \le m(i)
	\end{align*}
	unabhängige, $\cap$-stabile Familien.
	Dann sind auch
	\begin{align*}
		\G_i = \sigma(\F_{i,1},\dots, \F_{i,m(i)}), \quad 1 \le i \le n
	\end{align*}
	unabhängig.
\end{conclusion}

\begin{conclusion}
	$(\O,\F,\P)$ Wahrscheinlichkeitsraum, und
	\begin{align*}
		X_{ij}: \O \to E, \quad 1 \le i \le n, 1 \le j \le m(i)
	\end{align*}
	unabhängige Zufallsvariablen. Zudem seinen $f_i: E^{m(i)} \to \R$ messbar. Dann sind auch die Zufallsvariablen
	\begin{align*}
		f_i(X_{i1}, \dots, X_{im(i)}), \quad 1 \le i \le n
	\end{align*}
	unabhängig.
\end{conclusion}

\begin{example}
	$X_1, \dots, X_n$ unabhängige reelle Zufallsvariablen. Dann sind auch
	\begin{align*}
		Y_1 = X_1, Y_2 = X_2 + \cdots + X_n
	\end{align*}
	unabhängig.
\end{example}