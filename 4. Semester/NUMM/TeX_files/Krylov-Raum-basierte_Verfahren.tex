\section{\person{Krylov}-Raum-basierte Verfahren}

\subsection{\person{Krylov}-Räume}

Für $A\in\Rnn$, $r\in\Rn$ und $k\in\natur$ ist der $k$-te \begriff{Krylov-Raum} gegeben durch $\mathcal{K}_0=\{0\}$ und
\begin{align}
	\mathcal{K}_k(r,A) = \Span\{r,Ar,A^2r,...,A^{k-1}r\}\quad\text{ für } k>0\notag
\end{align}
Offenbar ist $\dim(\mathcal{K}_k(r,A))\le\min\{k,n\}$ für alle $k\in\natur$.

\begin{lemma}
	\proplbl{lemma_2_6}
	Es seien $A\in\Rnn$, $r\in\Rn\backslash\{0\}$ und $k\in\natur$ gegeben. Dann sind folgende Aussagen äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $\dim(\mathcal{K}_{k+1}(r,A))< k+1$
		\item $\mathcal{K}_k(r,A) = \mathcal{K}_{k+1}(r,A)$
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item (a) $\Rightarrow$ (b): Nach Voraussetzung gibt es $l\in\{1,...,k\}$ und $\alpha_0,...,\alpha_l\in\R$, so dass
		\begin{align}
			A^lr = \sum_{i=0}^{l-1} \alpha_iA^ir \notag
		\end{align}
		Multiplikation mit $A^{k-l}$ liefert
		\begin{align}
			A^kr = \sum_{i=0}^{l-1} \alpha_iA^{k-l+i}r\in\mathcal{K}_k(r,A)\notag
		\end{align}
		Also folgt $\mathcal{K}_k(r,A) = \mathcal{K}_{k+1}(r,A)$.
		\item (b) $\Rightarrow$ (a): Offensichtlich
	\end{itemize}
\end{proof}

\begin{proposition}
	\proplbl{satz_2_7}
	Es seien $A\in\Rnn$ regulär, $x^0\in\Rn$ mit $r^0=b-Ax^0\neq 0$ gegeben. Dann sind folgende Aussagen für $k\in\natur$ äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $\mathcal{K}_k(r^0,A) = \mathcal{K}_{k+1}(r^0,A)$
		\item $x^\ast = A^{-1}b\in x^0 + \mathcal{K}_k(r^0,A)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{itemize}
		\item (a) $\Rightarrow$ (b): Wegen \propref{lemma_2_6} gibt es $l\in\{0,...,k\}$ und $\mu_l,...,\mu_k\in\R$, so dass $\mu_l\neq 0$ und
		\begin{align}
			0 &= \sum_{i=l}^{k} \mu_iA^ir^0 \notag \\
			&= \mu_lA^lr^0 + \sum_{i=l+1}^{k} \mu_iA^ir^0 \notag
		\end{align}
		wobei der Summationsterm auf der rechten Seite entfällt, wenn $l=k$. Wegen der Regularität von $A$ kann man die Gleichung mit $A^{-(l+1)}$ multiplizieren. Für $l=k$ liefert dies $0=A^{-1}r^0 = A^{-1}b-x^0\in\mathcal{K}_k(r^0,A)$. Für $l<k$ folgt
		\begin{align}
			A^{-1}b - x^0 &= A^{-1}r \notag \\
			&= -\frac{1}{\mu_l} \sum_{i=l+1}^k \mu_iA^{i-l-1}r ^0 \notag \\
			&= -\frac{1}{\mu_l}\sum_{i=0}^{k-l-1} \mu_{i+l+1}A^ir^0\in\mathcal{K}_k(r^0,A) \notag
		\end{align}
		Somit gilt Aussage (b).
		\item (b) $\Rightarrow$ (a): Nach Voraussetzung gilt $x^\ast\in x_0 + \mathcal{K}_k(r^0,A)$. Durch Multiplikation mit $A$ folgt
		\begin{align}
			b&\in Ax^0 + A\mathcal{K}_k(r^0,A)\notag \\
			&= Ax^0 + \Span\{Ar^0,A^2r^0,A^3r^0,...,A^kr^0\}\notag
		\end{align}
		Also ist $r^0=b-Ax^0$ eine Linearkombination der Vektoren $Ar^0,A^2r^0,A^3r^0,...,A^kr^0$ und es gilt
		\begin{align}
			\dim(\mathcal{K}_{k+1}(r^0,A)) < k+1\notag
		\end{align}
		 \propref{lemma_2_6} liefert damit die Gültigkeit von Aussage (a).
	\end{itemize}
\end{proof}

\begin{remark}
	\proplbl{bemerkung_2_8}
	Offenbar gibt es $k^\ast\in\{1,...,n\}$, so dass Aussage (a) von \propref{satz_2_7} für $k^\ast$ zutrifft, aber für kein $k<k^\ast$ erfüllt ist. \propref{satz_2_7} zeigt daher, dass die exakte Lösung $x^\ast$ von $Ax=b$ in $x^0 + \mathcal{K}_{k^\ast}(r^0,A)$ liegt. Man kann also nun versuchen, eine Folge $\{x^k\}$ mit $x^k\in x^0 + \mathcal{K}_k(r^0,A)$ zu bestimmen, so dass $x^k$ das Gleichungssystem $Ax=b$ (geeignet) näherungsweise löst. Dazu werden in nächsten Abschnitt zwei grundlegende Ansätze angegeben (Minimum-Residuum und Galerkin).
\end{remark}

\subsection{Basisalgorithmen zur Lösung von $Ax=b$}

Für eine reguläre Matrix $B\in\Rnn$ ist durch
\begin{align}
	\norm{x}_B = \norm{Bx}_2\quad\text{für } x|in\Rn\notag
\end{align}
eine Vektornorm im $\Rn$ definiert. Unabhängig von der konkreten Wahl der regulären Matrix $B$ sind das lineare Gleichungssystem $Ax=b$ und die Minimierungs aufgabe
\begin{align}
	\frac{1}{2}\norm{b-Ax}_B^2\to\min\notag
\end{align}
offenbar äquivalent.

\begin{algorithm}[Minimum-Residuum Basisalgorithmus]
	\proplbl{algorithmus_2_9}
	Input: $x^0\in\R$, $A,B\in\Rnn$ regulär, $b\in\Rn$
\begin{lstlisting}
while %$r^k\neq 0$% do
 k = k + 1;
 compute %$x^k\in\Rn\text{ als Lösung von}$%
 %$\quad f_B(x) = \frac{1}{2}\Vert b-Ax\Vert^2_B\to\min\quad\text{bei } x\in x^0+\mathcal{K}_k(r^0,A)$%
 %$r^k$% = b - A%$x^k$%
enddo
\end{lstlisting}
	Output: $x^\ast=x^k$, $k^\ast=k$
\end{algorithm}

\begin{proposition}
	\proplbl{satz_2_10}
	\propref{algorithmus_2_9} ist wohldefiniert und bricht (bei exakter Arithmetik) nach höchstens $n$ (das heißt $k^\ast\le n$) it der exakten Lösung $x^\ast$ des Gleichungssystems $Ax=b$ ab.
\end{proposition}
\begin{proof}
	Die Funktion $f_B:\Rn\to\R$ (Zielfunktion der Optimierungsaufgabe) ist gleichmäßig konvex, da $A$ regulär ist. Die Menge $x^0+\mathcal{K}_k(r^0,A)$ (zulässiger Bereich  der Optimierungsaufgabe) ist außerdem nichtleer, abgeschlossen und konvex.  Nach einem bekannten Resultat der kontinuierlichen Optimierung besitzt die Optimierungsaufgabe eine eindeutige Lösung $x^k$. Somit ist \propref{algorithmus_2_9} wohldefiniert. Wegen $\mathcal{K}_k(r^0,A)=\mathcal{K}_{k+1}(r^0,A)$ spätestens für $k=n$ muss \propref{algorithmus_2_9} auf Grund von \propref{satz_2_7} nach höchstens $n$ Schritten mit der exakten Lösung $x^\ast$ abbrechen, vgl. auch \propref{bemerkung_2_8}
\end{proof}

\propref{algorithmus_2_9} ist somit ein direktes Verfahren. Jedoch besteht die praktische Bedeutung von Realisierungen dieses Verfahrens darin, dass man unter bestimmten Bedingungen bereits für $k\ll k^\ast$ eine brauchbare Näherung $x^k$ für $x^\ast$ erhalten kann. Dabei ist es von entscheidender Bedeutung, dass jeder Schritt des Algorithmus möglichst wenig Aufwand erfordert (etwa $\mathcal{O}(n)$ für gewisse schwach besetzte Matrizen). Analoges trifft auch für folgende Klasse von Algorithmen zu.

\begin{algorithm}[Galerkin Basisalgorithmus]
	\proplbl{algorithmus_2_11}
	Input: $x^0\in\R$, $A,B\in\Rnn$ regulär, $b\in\Rn$
	\begin{lstlisting}
%$r^0$% = b - A%$x^0$%;
k = 0;
while %$r^k\neq 0$% do
 k = k + 1;
 determine %Untervektorraum $\mathcal{L}_k$ von $\Rn$ mit $\dim(\mathcal{L}_k)=k$%;
 compute %$x^k\in x^0+\mathcal{K}_k(r^0,A)$ mit $b-Ax^k\perp\mathcal{L}_k$%;
 %$r^k$% = b - A%$x^k$%;
enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$, $k^\ast=k$
\end{algorithm}

Unter der Voraussetzung der Durchführbarkeit bricht \propref{algorithmus_2_11} nach höchstens $n$ Schritten mit der Lösung $x^\ast$ von $Ax=b$ ab, da $\mathcal{L}_n=\Rn$ und $b-Ax^n\perp\mathcal{L}_n$ somit $b-Ax^n=0$ impliziert.

\subsection{Das CG-Verfahren}

Es seien $A\in\Rnn$ und $b\in\Rn$ gegeben. Die Funktion $f:\Rn\to\R$ sei gegeben durch
\begin{align}
	f(x) = \frac{1}{2}x^TAx-b^Tx\quad\text{für } x\in\Rn\notag
\end{align}
Falls die Matrix $A\in\Rnn$ symmetrisch und positiv definit ist (und das wird beim CG-Verfahren immer vorausgesetzt werden), gibt es eine orthogonale Matrix $Q\in\Rnn$ und eine Diagonalmatrix $\Lambda=\diag(\lambda_1,...,\lambda_n)$ mit den $n$ positiven Eigenwerten $\lambda_1,...,\lambda_n$ von $A$, so dass
\begin{align}
	A = Q\Lambda Q^T\notag
\end{align}
Mit $\Lambda^{\sfrac{1}{2}}=\diag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})$ und $A^{\sfrac{1}{2}}=Q\Lambda^{\sfrac{1}{2}}Q^T$ gilt $A^{\sfrac{1}{2}}A^{\sfrac{1}{2}}=A$. Die Matrix $A^{\sfrac{1}{2}}$ wird als Wurzel von $A$ bezeichnet und ist symmetrisch und positiv definit. Es gibt keine weitere symmetrisch positiv definite Matrix, die Wurzel von $A$ ist.

\begin{lemma}
	\proplbl{lemma_2_12}
	Seien $A\in\Rnn$ symmetrisch und positiv definit, $b\in\Rn$, $G\subseteq\Rn$ eine abgeschlossene nichtleere Menge und $x^\ast=A^{-1}b$. Dann sind folgende Aussagen äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $y$ minimiert $f$ auf $G$
		\item $y$ minimiert $x\mapsto\norm{x-x^\ast}_{A^{\sfrac{1}{2}}}$ auf $G$
		\item $y$ minimiert $x\mapsto\norm{b-Ax}_{A^{-\sfrac{1}{2}}}$ auf $G$
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item (a) $\Rightarrow$ (b): Für jedes $x\in\Rn$ gilt
		\begin{align}
			\norm{x-x^\ast}^2_{A^{\sfrac{1}{2}}} &= \norm{A^{\sfrac{1}{2}}(x-x^\ast)}^2 \notag \\
			&= (x-x^\ast)^TA(x-x^\ast)\notag \\
			&= x^TAx-2x^TAx^\ast+(x^\ast)^TAx^\ast \notag \\
			&= x^TAx-2x^Tb+b^Tx^\ast\notag \\
			&= 2f(x) + b^Tx^\ast \notag
		\end{align}
		Da $b^Tx^\ast$ nicht von $x$ abhängt, ist eine Minimalstelle $y$ von $f(x)$ auf $G$ auch eine Minimalstelle von $x\mapsto\norm{x-x^\ast}_{A^{\sfrac{1}{2}}}$ auf $G$ und umgekehrt.
		\item (b) $\Rightarrow$ (c): Für jedes $x\in\Rnn$ gilt
		\begin{align}
			\norm{x-x^\ast}^2_{A^{\sfrac{1}{2}}} &= (x-x^\ast)^TA(x-x^\ast) \notag \\
			&= (x-x^\ast)^TAA^{-1}A(x-x^\ast) \notag \\
			&= (A(x-x^\ast))^TA^{-1}A(x-x^\ast) \notag \\
			&= (Ax-b)^TA^{-1}(Ax-b) \notag \\
			&= \norm{A^{-\sfrac{1}{2}}(b-Ax)}^2 \notag \\
			&= \norm{b-Ax}_{A^{-\sfrac{1}{2}}}^2 \notag
		\end{align}
		Damit folgt die Behauptung unmittelbar.
	\end{itemize}
\end{proof}

Das CG-Verfahren ergibt sich nn formal aus \propref{algorithmus_2_9} (Minimum-Residuum Basisalgorithmus), indem dort $B=A^{-\sfrac{1}{2}}$ gesetzt wird. \propref{lemma_2_12} zeigt uns dann, dass das Teilproblem, also
\begin{align}
	\label{2_13}
	\frac{1}{2}\norm{b-Ax}^2_{A^{-\sfrac{1}{2}}}\to\min\quad\text{bei } x\in x^0+\mathcal{K}_k(r^0,A)
\end{align}
zur Aufgabe
\begin{align}
	\label{2_14}
	\frac{1}{2}x^TAx-b^Tx\to\min\quad\text{bei } x\in x^0+\mathcal{K}_k(r^0,A)
\end{align}
äquivalent ist. Es stellt sich nun die Frage, wie diese Teilprobleme effizient zu lösen sind. Dazu gibt nachfolgendes Lemma einen Hinweis. Zu seiner Formulierung benötigen wir noch

\begin{definition}[$A$-konjugiert, $A$-orthogonal]
	Seien $A\in\Rnn$ symmetrisch und positiv definit und $k\in\natur$ mit $k\le n$. Dann nennt man Vektoren $d^0,...,d^{k-1}\in\Rn\backslash\{0\}$ mit der Eigenschaft
	\begin{align}
		(d^i)^TAd^j=0\quad\forall\text{ Paare } (i,j) \text{ mit } i,j\in\{0,...,k-1\} \text{ und } i\neq j\notag
	\end{align}
	\begriff{$A$-konjugiert} oder \begriff{$A$-orthogonal}.
\end{definition}

Die lineare Hülle von Vektoren $d^0,...,d^{k-1}\in\R^k$ soll durch 
\begin{align}
	\mathcal{D}_k=\Span\{d^0,...,d^{k-1}\}\notag
\end{align}
bezeichnet werden, wobei durch den Kontext klar ist, welche Vektoren $d^0,...,d^{k-1}$ gemeint sind.

\begin{lemma}
	\proplbl{lemma_2_14}
	Sei $A\in\Rnn$ symmetrisch und positiv definit. Weiter seien $x^0\in\Rn$ und $A$-konjugierte Vektoren $d^0,...,d^{n-1}\in\Rn\backslash\{0\}$ gegeben. Dann besitzt die Optimierungsaufgabe
	\begin{align}
		\label{2_15}
		\frac{1}{2}x^TAx-b^Tx\to\min\quad\text{bei } x\in x^0+\mathcal{D}_k
	\end{align}
	eine eindeutig bestimmte Lösung $x^k$ für $k=1,...,n$. Dabei gilt für $k=0,...,n-1$
	\begin{align}
		\label{2_16}
		x^{k+1} = x^k + t_kd^k\quad\mit\quad t_k = \frac{(r^k)^Td^k}{(d^k)^TAd^k}\quad\text{und}\quad r^k=b-Ax^k
	\end{align}
	und
	\begin{align}
		\label{2_17}
		(r^{k+1})^Td^j = 0\quad\text{für } j=0,...,k
	\end{align}
\end{lemma}
\begin{proof}
	Zunächst sei die Funktion $\phi:\R^k\to\R$ für ein $k\in\{1,...,n\}$ wie folgt definiert:
	\begin{align}
		\phi(\alpha) = f\left(x ^0+\sum_{i=0}^{k-1}\alpha_id^i\right)\quad\forall\alpha\in\R^k\notag
	\end{align}
	Offenbar ist dann jede Lösung $\alpha^\ast$ der freien Optimierungsaufgabe 
	\begin{align}
		\label{2_18}
		\phi(\alpha)\to\min
	\end{align}
	vermöge
	\begin{align}
		\label{2_19}
		x^k = x^0+\sum_{i=0}^{k-1}\alpha^\ast_id^i
	\end{align}
	eine Lösung von \cref{2_15} und entsprechend umgekehrt. Wir untersuchen daher nun \cref{2_18}. Für $\phi(\alpha)$ erhält man
	\begin{align}
		\phi(\alpha) = \frac{1}{2}\left(x^0+\sum_{i=0}^{k-1}\alpha_id^i\right)^TA\left(x^0+\sum_{i=0}^{k-1}\alpha_id^i\right) - b^T\left(x^0+\sum_{i=0}^{k-1}\alpha_id^i\right)\notag
	\end{align}
	Da $d^0,...,d^{k-1}$ $A$-konjugiert sind, folgt
	\begin{align}
		\phi(\alpha) &= \frac{1}{2}\sum_{i=0}^{k-1}\alpha_i^2(d^i)^TAd^i + (Ax^0-b)^T\left(x^0 + \sum_{i=0}^{k-1}\alpha_i d^i\right) - \frac{1}{2}(x^0)^TAx^0 \notag \\
		&= \frac{1}{2}\sum_{i=0}^{k-1}\alpha_i^2(d^i)^TAd^i - \sum_{i=0}^{k-1}\alpha_i(r^0)^Td^i + f(x^0)\notag
	\end{align}
	Jede Lösung $\alpha^\ast$ von \cref{2_18} muss der notwendigen Optimalitätsbedingung $\nabla\phi(\alpha)=0$ genügen. Wegen der Konvexität von $\phi$ (beachte $\nabla^2\phi = A$ ist positiv definit) ist diese Bedingung auch hinreichend. Aus $\nabla\phi(\alpha^\ast)=0$ folgt
	\begin{align}
		0=\frac{\partial \phi}{\partial\alpha_i}(\alpha^\ast) =\alpha^\ast_i(d^i)^TAd^i-(r^0)^Td^i\quad\text{für } i=0,...,k-1\notag 
	\end{align}
	und damit (beachte positive Definitheit von $A$) nach Umstellen
	\begin{align}
		\label{2_20}
		\alpha_i^\ast = \frac{(r^0)^Td^i}{(d^i)^TAd^i}\quad\text{für } i=0,...,k-1
	\end{align}
	Also besitzt \cref{2_18} genau eine Lösung $\alpha^\ast$. Entsprechend \cref{2_15} eine eindeutige Lösung, die sich aus \cref{2_19} ergibt. Man sieht nun sofort, dass der Wert $\alpha_i^\ast$ nicht vom gewählten $k$ abhängig. Mit \cref{2_19} folgt daraus
	\begin{align}
		\label{2_21}
		x^{k+1} -x^k = \alpha_k^\ast d^k\quad\text{für } k=0,...,n-1
	\end{align}
	Damit erhält man für $i\in\{1,...,k-1\}$
	\begin{align}
		(r^0)^T d^i &= \left(r^i + \sum_{j=1}^{i} (r^{j-1}-r^j)\right)^Td^i \notag \\
		&= \left(r^i + \sum_{j=1}^i (-Ax^{j-1} + Ax^j)\right)^T d^i \notag \\
		&= (r^i)^Td^i + \sum_{j=1}^i (x^j-x^{j-1})^T Ad^i \notag \\
		&= (r^i)^Td^i + \sum_{j=1}^i \alpha^\ast_{j-1}(d^{j-1})^TAd^i \notag
	\end{align}
	und weiter unter Ausnutzung der $A$-Konjugiertheit der Vektoren $d^0,...,d^{n-1}$
	\begin{align}
		(r^0)^Td^i = (r^i)^Td^i\notag
	\end{align}
	Setzt man dies in die Darstellung von $\alpha_i^\ast$ ein ,so folgt wegen \cref{2_21} und \cref{2_20} Teilbehauptung \cref{2_16}.
	
	Die letzte Teilbehauptung \cref{2_17} ergibt sich für $j=k$ aus
	\begin{align}
		\label{2_22}
		\begin{split}
		(r^{k+1})^Td^k &= (b-Ax^{k+1})^Td^k \\
		&= (b-Ax^k-t_kAd^k)^Td^k \\
		&= (r^k)^Td^k-t_k(d^k)^TAd^k \\
		&= 0
		\end{split}
	\end{align}
	wobei die letzte Gleichung mit Hilfe der Darstellung von $t_k$ in \cref{2_16} klar wird. Für $j\in\{0,...,k-1\}$ zeigt man \cref{2_17} schließlich unter Benutzung von \cref{2_22} und der $A$-Konjugiertheit der $d^0,...,d^{n-1}$
	\begin{align}
		(r^{k+1})^Td^j &= (r^{j+1})^Td^j + \sum_{i=j+1}^k (r^{i+1}-r^i)^Td^j \notag \\
		&= \sum_{i=j+1}^k (b-Ax^{i+1}-b+Ax^i)^Td^j \notag \\
		&= \sum_{i=j+1}^k (-t_iAd^i)^Td^j \notag \\
		&= 0 \notag
	\end{align}
\end{proof}

Um nun die Teilprobleme \cref{2_14} mit Hilfe von \propref{lemma_2_14} lösen zu können, bietet es an, die Vektoren $d^0,d^1,...$ möglichst so zu erzeugen, dass sie einerseits $A$-konjugiert sind und andererseits $\mathcal{K}_k(r^0,A)=\mathcal{D}_k$ für $k=1,2,...$ gilt. Zuerst setzt man dazu
\begin{align}
	d^0=r^0\notag
\end{align}
so dass $\mathcal{K}_1(r^0,A)=\Span\{r^0\}=\mathcal{D}_1$ folgt. Zur Abkürzung sei noch
\begin{align}
	\mathcal{R}_k=\Span\{r^0,...,r^{k-1}\}\notag
\end{align}
definiert. Wir nehmen nun an, dass für ein $k\in\{1,...,n-1\}$ bereits
\begin{align}
	\label{2_23}
	\mathcal{R}_k = \mathcal{K}_k(r^0,A)=\mathcal{D}_k
\end{align}
mit
\begin{align}
	\label{2_24}
	A\text{-konjugierten Vektoren } d^0,...,d^{k-1}\in\Rn\backslash\{0\}
\end{align}
erfüllt ist. Falls $r^k\neq 0$ soll nun gezeigt werden, dass \cref{2_23} und \cref{2_24} mit einem geeignet zu bestimmenden $d^k$ auch für $k+1$ anstelle von $k$ gelten.

Offenbar zeiht \cref{2_16} in \propref{lemma_2_14}
\begin{align}
	r^k &= b-Ax^k \notag \\
	&= b-A(x^{k-1}+t_{k-1}d^{k-1}) \notag \\
	&= r^{-1} - t_{k-1}Ad^{k-1} \notag
\end{align}
nach sich. Da
\begin{align}
	r^{k-1}&\in\mathcal{K}_k(r^0,A)\subseteq\mathcal{K}_{k+1}(r^0,A)\text{ und } \notag \\
	Ad^{k-1}&\in A\mathcal{K}_k(r^0,A)\subseteq\mathcal{K}_{k+1}(r^0,A) \notag
\end{align}
folgt
\begin{align}
	r^k\in\mathcal{K}_{k+1}(r^0,A)\notag
\end{align}
Wegen $r^k\in\mathcal{R}_{k+1}\backslash\{0\}$ hat man weiter
\begin{align}
	\mathcal{K}_{k+1}(r^0,A) = \mathcal{R}_{k+1}\notag
\end{align}
Um auch $\mathcal{K}_{k+1}(r^0,A)=\mathcal{D}_{k+1}$ zu erreichen, muss man also
\begin{align}
	\mathcal{D}_{k+1} = \Span\{\mathcal{D}_k,r^k\}\notag
\end{align}
setzen. Zur Bestimmung eines Vektors $d^k\neq 0$ derart, dass $\mathcal{D}_{k+1} = \Span\{\mathcal{D}_k,d^k\}$ und $d^0,...,d^k$ $A$-konjugiert sind, kann (analog zum Orthogonalisierungsverfahren nach \person{Gram-Schmidt}) der Ansatz
\begin{align}
	\label{2_25}
	d^k=r^k+\sum_{i=0}^{k-1}\beta_id^i
\end{align}
verwendet werden. Wegen \cref{2_17} in \propref{lemma_2_14} und der Voraussetzung $r^k\neq 0$ hat man zunächst $d^k\neq 0$. Die gewünschte $A$-Konjugiertheit liefert nun die Bedingungen
\begin{align}
	0&=(d^k)^TAd^j \notag \\
	&= (r^k)^TAd^j+\sum_{i=0}^{k-1}\beta_i(d^i)^TAd^j\quad\text{für } j=0,...,k-1\notag
\end{align}
Auf Grund der entsprechend \cref{2_23} vorausgesetzten $A$-Konjugiertheit von $d^0,...,d^{k-1}$ ist dies äquivalent zu
\begin{align}
	0=(r^k)^TAd^j+\beta_j(d^j)^TAd^j\quad\text{für } j=0,...,k-1\notag
\end{align}
Wegen der positiven Definitheit von $A$ hat man
\begin{align}
	\label{2_26}
	\beta_j = -\frac{(r^k)^TAd^j}{(d^j)^TAd^j}\quad\text{für } j=0,...,k-1
\end{align}
Für $j\in\{0,...,k-1\}$ ergibt sich für den Zähler des Bruches in \cref{2_26}
\begin{align}
	(r^k)^TAd^j &= \frac{1}{t_j}(r^k)^TA(x^{j+1}-x^j) \notag \\
	&= \frac{1}{t_j}(r^k)^T(r^j-r^{j+1}) \notag
\end{align}
Da $r^j\in\mathcal{R}_k=\mathcal{D}_k=\Span\{d^0,...,d^{k-1}\}$ nach \cref{2_23} für $j\in\{0,...,k-1\}$ gilt, liefert \cref{2_17} in \propref{lemma_2_14}, dass
\begin{align}
	(r^k)Ad^j = \begin{cases}
		0 & \text{für } j=0,...,k-2 \\ \frac{-1}{t_{k-1}}(r^k)^Tr^k & \text{für } j=k-1 
	\end{cases}\notag
\end{align}
Somit und wegen \cref{2_16} folgt aus \cref{2_26}
\begin{align}
	\label{2_27}
	\beta_0=\dots=\beta_{k-2}=0\quad\text{und}\quad\beta_{k-1}=\frac{\norm{r^k}_2^2}{\norm{r^{k-1}}^2_2}
\end{align}
sowie mit \cref{2_25}
\begin{align}
	\label{2_28}
	d^k=r^k+\beta_{k-1}d^{k-1}
\end{align}
Da \cref{2_23} und \cref{2_24} trivialerweise auch für $k=0$ richtig waren, ist gezeigt, dass \cref{2_23} und \cref{2_24} mit \cref{2_27} und \cref{2_28} für $k=1,2,...$ gelten, sofern $r^{k-1}\neq 0$.

Multipliziert man $d^k$ in \cref{2_28} von links mit $(r^k)^T$ und berücksichtigt man \cref{2_17}, so ergibt sich noch folgenden Vereinfachung für $t_k$ aus \cref{2_16}:
\begin{align}
	\label{2_29}
	t_k &= \frac{(r^k)^Td^k}{(d^k)^TAd^k} \notag \\
	&= \frac{\norm{r^k}_2^2+\beta_{k-1}(r^k)^Td^{k-1}}{(d^k)^TAd^k} \notag \\
	&= \frac{\norm{r^k}_2^2}{(d^k)^TAd^k} \notag
\end{align}
Mit \cref{2_27}, \cref{2_28}, \cref{2_29} und unter Beachtung der Äquivalenz der Teilprobleme \cref{2_13} und \cref{2_14} ergibt sich mit $B=A^{-\sfrac{1}{2}}$ folgende Realisierung des Algorithmus \propref{algorithmus_2_9}

\begin{algorithm}[CG-Verfahren]
	\proplbl{algorithmus_2_15}
	Input: $x^0\in\Rn$, $A\in\Rnn$ symmetrisch und positiv definit, $b\in\Rn$
	\begin{lstlisting}
%$d^0$% = %$r^0$% = b - A%$x^0$%;
k = 0;
while %$r^k\neq 0$% do
 %$t_k$% = %$\frac{\Vert r^k\Vert_2^2}{(d^k)^T Ad^k}$%;
 %$x^{k+1}$% = %$x^k$% + %$t_kd^k$%;
 %$r^{k+1}$% = b - A%$x^{k+1}$% = %$r^k$% - %$t_kAd^k$%;
 %$\beta_k$% = %$\frac{\Vert r^{k+1}\Vert_2^2}{\Vert r^k\Vert_2^2}$%;
 %$d^{k+1}$% = %$r^{k+1}$% - %$\beta_kd^k$%;
 k = k + 1;
enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$
\end{algorithm}
Der Rechenaufwand pro Schritt des CG-Verfahrens ist durch den Aufwand zur Berechnung des Matrix-mal-Vektor-Produkts $Ad^k$ (sowie einige weitere Operationen wie Skalarprodukte) gegeben. Falls die Matrix $A$ geeignet schwach besetzt ist, kommt man daher mit $\mathcal{O}(n)$ Operationen pro Schritt des CG-Verfahrens aus. Von besonderer Bedeutung zur Erreichung dieses geringen Aufwandes ist die sehr einfache Bestimmung der $A$-konjugierten Basisvektoren $d^0,d^1,...$ der \person{Krylov}-Räume $\mathcal{K}_1(r^0,A),\mathcal{K}_2(r^0,A),...$. Obwohl wegen \propref{satz_2_10} bekannt ist, dass das CG-Verfahren (bei exakter Arithmetik) nach höchstens $n$ Schritten mit einer Lösung von $Ax=b$ abbricht, gilt das Interesse hier der Frage, wie sich der Fehler der Näherung $x^k$ gegenüber einer Lösung $x^\ast$ abschätzen lässt. Dies ist besonders für $k\ll n$ von praktischem Interesse.

\subsection{Fehlerverhalten des CG-Verfahrens}

\begin{definition}[$n$-tes \person{Tschebyschow}-Polynom]
	\proplbl{definition_2_16}
	Für $n\in\natur$ wird die durch
	\begin{align}
		T_n(x) = \begin{cases}
			\frac{1}{2}\left[\left(x+i\sqrt{1-x^2}\right)^n+\left(x-i\sqrt{1-x^2}\right)^n\right]&\text{falls } \abs{x}\le 1 \\
			\frac{1}{2}\left[\left(x+i\sqrt{x^2-1}\right)^n+\left(x-i\sqrt{x^2-1}\right)^n\right]&\text{falls } \abs{x}> 1 \\
		\end{cases}\notag
	\end{align}
	definierte Funktion $T_n:\R\to\R$ als $n$-tes \begriff{\person{Tschebyschow}-Polynom} bezeichnet.
\end{definition}

Für die Sinnhaftigkeit der Definition und weitere Darstellungen der \person{Tschebyschow}-Polynome sei auf eine entsprechende Übungsaufgabe verwiesen, insbesondere sei noch bemerkt, dass
\begin{align}
	\label{2_30}
	T_n(x)=\cos(n\cdot \arccos(x))\quad\forall x\in[-1,1]
\end{align}
für jedes $n\in\natur$ erfüllt ist.

\begin{lemma}
	\proplbl{lemma_2_17}
	Für jedes $n\in\natur$ gilt
	\begin{align}
		T_n\left(\frac{\kappa +1}{\kappa -1}\right) \ge \frac{1}{2}\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^n\quad\forall\kappa >1\notag
	\end{align}
\end{lemma}
\begin{proof}
	Sei $x=\frac{\kappa +1}{\kappa -1}$. Dann gilt $x>1$ wegen $\kappa >1$. Mit \propref{definition_2_16} folgt
	\begin{align}
		2T_n(x)\ge \left(x+\sqrt{x^2-1}\right)^n\notag
	\end{align}
	Die Behauptung ergibt sich nun, da
	\begin{align}
		x+\sqrt{x^2-1} &= \frac{\kappa +1+\sqrt{(\kappa +1)^2-(\kappa -1)^2}}{\kappa-1} \notag \\
		&= \frac{\kappa+2\sqrt{\kappa}+1}{\kappa-1} \notag \\
		&= \frac{(\sqrt{\kappa}+1)^2}{(\sqrt{\kappa}-1)(\sqrt{\kappa}+1)} \notag \\
		&= \frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1} \notag
	\end{align}
\end{proof}

Es sei daran erinnert, dass die Konditionszahl einer regulären Matrix $A$ in der Spektralnorm durch 
\begin{align}
	\cond_2(A) &= \norm{A}_2\norm{A^{-1}}_2 \notag \\
	&= \sqrt{\lambda_{max}(A^TA)}\sqrt{\lambda_{max}((A^T)^{-1}A^{-1})} \notag \\
	&= \sqrt{\frac{\lambda_{max}(A^TA)}{\lambda_{min})(A^TA)}} \notag 
\end{align}
gegeben ist, wobei $\lambda_{max}$ bzw. $\lambda_{min}$ den größten bzw. kleinsten Eigenwert der jeweiligen Matrix bezeichnen. Falls $A$ sogar symmetrisch und positiv definit ist, folgt
\begin{align}
	\cond_2(A) = \frac{\lambda_{max}(A)}{\lambda_{min}(A)}\notag
\end{align}

\begin{proposition}
	Es seien $A\in\Rnn$ symmetrisch und positiv definit und $b\in\Rn$. Weiter seien $x^\ast=A^{-1}b$, $\kappa=\cond_2(A)$ und $\{x^k\}$ eine durch \propref{algorithmus_2_15} (CG-Verfahren) erzeugte Folge. Dann gilt
	\begin{align}
		\norm{x^k-x^\ast}_{A^{\sfrac{1}{2}}} \le 2\left(\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}\right)^k\norm{x^0 - x^\ast}_{A^{\sfrac{1}{2}}}\notag
	\end{align}
	für $k=1,2,...$.
\end{proposition}
\begin{proof}
	Es bezeichne $e^k=x^k-x^\ast$. Wegen $r^0=b-Ax^0=Ax^\ast-Ax^0=-Ae^0$ folgt
	\begin{align}
		A^jr^0 = -A^{j+1}e^0\notag
	\end{align}
	für alle $j\in\natur$ und damit
	\begin{align}
		\label{2_31}
		\begin{split}
			\mathcal{K}_k(r^0,A) &= \Span\{r^0,...,A^{k-1}r^0\} \\
			&= \Span\{Ae^0,...,A^ke^0\}
		\end{split}
	\end{align}
	Berücksichtigt man, dass $x^k$ Lösung von \cref{2_14} ist, so liefert die Äquivalenz (a) $\Leftrightarrow$ (b) in \propref{lemma_2_12}
	\begin{align}
		\norm{e^k}_{A^{\sfrac{1}{2}}} &= \norm{x^k-x^\ast}_{A^{\sfrac{1}{2}}} \notag \\
		&= \min_{x\in x^0+\mathcal{K}_k(r^0,A)}\norm{x-x^\ast}_{A^{\sfrac{1}{2}}} \notag 
	\end{align}
	Mit \cref{2_31} erhält man daraus weiter
	\begin{align}
		\label{2_32}
		\begin{split}
		\norm{e^k}_{A^{\sfrac{1}{2}}} &= \min_{z\in\mathcal{K}_k(r^0,A)}\norm{x^0-z-x^\ast}_{A^{\sfrac{1}{2}}} \\
		&= \min_{\alpha_1,...,\alpha_k}\norm{e^0+\sum_{i=1}^k \alpha_iA^ie^0}_{A^{\sfrac{1}{2}}} \\
		&= \min_{p\in\Pi^0_k} \norm{p(A)e^0}_{A^{\sfrac{1}{2}}}
		\end{split}
	\end{align}
	wobei $\Pi_k^0$ die Menge aller Polynome $p$ mit dem Höchstgrad $k$ und $p(0)=1$ bezeichnet. Da $A$ symmetrisch ist, gibt es eine Orthogonalbasis des $\Rn$ aus Eigenvektoren $v^1,...,v^n$ von $A$. Also existieren $\gamma_1,...,\gamma_n\in\R$, so dass
	\begin{align}
		\label{2_33}
		e^0 = \sum_{i=1}^n \gamma_iv^i
	\end{align}
	Bezeichnet man mit $\lambda_i$ den zu $v^i$ gehörenden Eigenwert, so ergibt sich
	\begin{align}
		\norm{e^0}^2_{A^{\sfrac{1}{2}}} &= (e^0)^TAe^0 \notag \\
		&= \left(\sum_{i=1}^n \gamma_iv^i\right)^T\left(\sum_{i=1}^n\gamma_iAv^i\right) \notag \\
		&= \left(\sum_{i=1}^n \gamma_iv^i\right)^T\left(\sum_{i=1}^n\gamma_i\lambda_iv^i\right) \notag
	\end{align}
	und schließlich (mit der Orthogonalität der Eigenvektoren)
	\begin{align}
		\label{2_34}
		\norm{e^0}^2_{A^{\sfrac{1}{2}}} = \sum_{i=1}^n\gamma_i^2\lambda_i
	\end{align}
	Man überlegt sich nun, dass für jedes $p\in\Pi_k^0$
	\begin{align}
		\label{2_35}
		\begin{split}
			p(A)v^i &= \mathbbm{1}v^i + \sum_{j=1}^k\alpha_jA^jv^i  \\
			&= v^i + \sum_{j=1}^k\alpha_j(\lambda_i)^jv^i  \\
			&= \left(1+\sum_{j=1}^k\alpha_j(\lambda_i)^j\right)v^i  \\
			&= p(\lambda_i)v^i 
		\end{split}
	\end{align}
	das heißt die Eigenwerte von $p(A)$ sind durch $p(\lambda_1),...,p(\lambda_n)$ gegeben. Mit \cref{2_33} folgt
	\begin{align}
		\norm{p(A)e^0}^2_{A^{\sfrac{1}{2}}} &= \left(p(A)e^0\right)^TAp(A)e^0 \notag \\
		&= \left(\sum_{i=1}^n \gamma_ip(A)v^i\right)^T\left(\sum_{i=1}^n\gamma_iAp(A)v^i\right)\notag
	\end{align}
	und weiter mit \cref{2_35}
	\begin{align}
		\norm{p(A)e^0}^2_{A^{\sfrac{1}{2}}} &= \left(\sum_{i=1}^n\gamma_ip(\lambda_i)v^i\right)^T \left(\sum_{i=1}^n\gamma_i\lambda_ip(\lambda_i)v^i\right) \notag \\
		&= \sum_{i=1}^n \gamma_i^2p(\lambda_i)^2\lambda_i \notag
	\end{align}
	Aus \cref{2_32} ergibt sich damit
	\begin{align}
		\norm{e^k}_{A^{\sfrac{1}{2}}} &= \min_{p\in\Pi^0_k}\norm{p(A)e^0}_{A^{\sfrac{1}{2}}} \notag \\
		&= \min_{p\in\Pi^0_k}\left(\sum_{i=1}^n\gamma_i^2p(\lambda_i)^2\lambda_i\right)^{\sfrac{1}{2}} \notag
	\end{align}
	Wegen \cref{2_34} und mit $\lambda_{min}=\lambda_{min}(A)$ sowie $\lambda_{max}=\lambda_{max}(A)$ hat man
	\begin{align}
		\label{2_36}
		\begin{split}
			\norm{e^k}_{A^{\sfrac{1}{2}}} &\le \min_{p\in\Pi^0_k}\max_{\lambda\in[\lambda_{min},\lambda_{max}]}\abs{p(\lambda)}\left(\sum_{i=1}^n\gamma_i^2\lambda_i\right)^{\sfrac{1}{2}}  \\
			&=\norm{e^0}_{A^{\sfrac{1}{2}}}\left(\min_{p\in\Pi^0_k}\max_{\lambda\in[\lambda_{min},\lambda_{max}]}\abs{p(\lambda)}\right) 
		\end{split}
	\end{align}
	Um den Term $(\dots)$ auf der rechten Seite von \cref{2_36} weiter abzuschätzen, sei zunächst bemerkt, dass im Fall $\lambda_{min}=\lambda_{max}$ dieser Term gleich 0 ist (man wähle $p(\lambda)=(\lambda_{max} - \lambda)\lambda_{max}^{-1}$). Also können wir $\lambda_{max}>\lambda_{min}$ annehmen und wählen dann $q\in\Pi_k^0$ speziell durch
	\begin{align}
		q(\lambda) = \frac{T_k(l(\lambda))}{T_k(l(0))}\quad\text{mit}\quad l(\lambda) = \frac{2\lambda-(\lambda_{max}+\lambda_{min})}{\lambda_{min}-\lambda_{max}}\notag
	\end{align}
	wobei $T_k$ das $k$-te \person{Tschebyschow}-Polynom ist. Die Funktion $l:\R\to\R$ ist offenbar monoton fallend mit $l(\lambda_{min})=1$ und $l(\lambda_{max})=-1$. Wegen \cref{2_30} gilt daher
	\begin{align}
		\max_{\lambda\in[\lambda_{min},\lambda_{max}]}\abs{T_k(l(\lambda))}\le 1\notag
	\end{align}
	Mit \propref{lemma_2_17} folgt noch
	\begin{align}
		T_k(l(0)) &= T_k\left(\frac{\lambda_{max}+\lambda_{min}}{\lambda_{max}-\lambda_{min}}\right) \notag \\
		&= T_k\left(\frac{\frac{\lambda_{max}}{\lambda_{min}}+1}{\frac{\lambda_{max}}{\lambda_{min}}-1}\right) \notag \\
		&= T_k\left(\frac{\kappa+1}{\kappa-1}\right) \notag \\
		&\ge \frac{1}{2}\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k\notag
	\end{align}
	Somit hat man
	\begin{align}
		\min_{p\in\Pi^0_k}\left(\max_{\lambda\in[\lambda_{min},\lambda_{max}]}\abs{p(\lambda)}\right) &\le \max_{\lambda\in[\lambda_{min},\lambda_{max}]}\abs{q(\lambda)} \notag \\
		&\le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k\notag
	\end{align}
	und wegen \cref{2_36} die Behauptung.
\end{proof}

\begin{proposition}
	Es seien $A\in\Rnn$ symmetrisch und positiv definit und $b\in\Rn$. Wenn $A$ genau $m$ paarweise verschiedene Eigenwerte besitzt, so bricht \propref{algorithmus_2_15} (bei exakter Rechnung) nach höchstens $m$ Schritten mit der Lösung $x^\ast$ des Gleichungssystems $Ax=b$ ab.
\end{proposition}
\begin{proof}
	Übungsaufgabe
\end{proof}

\subsection{Vorkonditionierung}

\begin{algorithm}[CG-Verfahren für $\tilde{A}\tilde{x}=\tilde{b}$]
	\proplbl{algorithmus_2_20}
	Input: $\tilde{x}^0\in\Rn$, $\tilde{A}\in\Rnn$ symmetrisch und positiv definit, $\tilde{b}\in\Rn$
	\begin{lstlisting}
	%$\tilde{d}^0$% = %$\tilde{r}^0$% = %$\tilde{b} - \tilde{A}\tilde{x}^0$%;
	k = 0;
	while %$\tilde{r}^k\neq 0$% do
	%$\tilde{t}_k$% = %$\frac{\Vert \tilde{r}^k\Vert_2^2}{(\tilde{d}^k)^T \tilde{A}\tilde{d}^k}$%;
	%$\tilde{x}^{k+1}$% = %$\tilde{x}^k$% + %$\tilde{t}_k\tilde{d}^k$%;
	%$\tilde{r}^{k+1}$% = %$\tilde{b} - \tilde{A}\tilde{x}^{k+1}$% = %$\tilde{r}^k$% - %$\tilde{t}_k\tilde{A}\tilde{d}^k$%;
	%$\tilde{\beta}_k$% = %$\frac{\Vert \tilde{r}^{k+1}\Vert_2^2}{\Vert \tilde{r}^k\Vert_2^2}$%;
	%$\tilde{d}^{k+1}$% = %$\tilde{r}^{k+1}$% - %$\tilde{\beta}_k\tilde{d}^k$%;
	k = k + 1;
	enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$
\end{algorithm}

\begin{algorithm}[Vorkonditioniertes CG-Verfahren]
	\proplbl{algorithmus_2_21}
	Input: $x^0\in\Rn$, $A\in\Rnn$ symmetrisch und positiv definit, $b\in\Rn$
	\begin{lstlisting}
%$r^0$% = b - A%$x^0$%;
compute %$z^0$ als Lösung von $Pz = r^0$% 
%$d^0$% = %$z^0$%
k = 0;
while %$r^k\neq 0$% do
 %$t_k$% = %$\frac{(r^k)^T}{(d^k)^T Ad^k}$%;
 %$x^{k+1}$% = %$x^k$% + %$t_kd^k$%;
 %$r^{k+1}$% = %$r^k$% - %$t_kAd^k$%;
 compute %$z^{k+1}$ als Lösung von $Pz = r^{k+1}$%
 %$\beta_k$% = %$\frac{(r^{k+1})^T z^{k+1}}{(r^k)^T z^k}$%;
 %$d^{k+1}$% = %$z^{k+1}$% + %$\beta_kd^k$%;
 k = k + 1;
enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$
\end{algorithm}

\subsection{Ausblick und Anmerkungen}

Es gibt zahlreiche weitere sogenannte \person{Krylov}-Raum basierte Verfahren. Auf einige dieser Methoden wird hier kurz hingewiesen.
\begin{itemize}
	\item \textbf{CGNR-Verfahren für reguläre Matrizen:} Da $A$ regulär ist, muss das System $Ax=b$ äquivalent zu $A^TAx=A^Tb$ sein. Offenbar ist $A^TA$ eine symmetrische und positiv definite Matrix. Damit kann das CG-Verfahren prinzipiell auf $A^TAx=A^Tb$ angewendet werden. Dieses Vorgehen wird als CGNR-Verfahren bezeichnet, dabei gilt jedoch $\cond_2(A^TA)=\cond_2(A)^2$.
	\item \textbf{GMRES-Verfahren für reguläre Matrizen:} Das GMRES-Verfahren von \person{Saad} und \person{Schultz} kann dann als Spezialfall des Minimum-Residuum Basisalgorithmus (\propref{algorithmus_2_9}) aufgefasst werden, wenn dort $B=\mathbbm{1}$ gesetzt wird. Die dabei notwendige Erzeugung und Abspeicherung einer Orthonormalbasis der \person{Krylov}-Räume $\mathcal{K}_k(r^0,A)$ ist zu aufwendig, so dass man zu einem GMRES$(m)$-Verfahren übergeht, dass nach $m$ Schritten und einem Restart mit der letzten erhaltenen Iterierten als Startvektor durchführt und dieses Verfahren ggf. mehrfach wiederholt.
	\item \textbf{MINRES-Verfahren für symmetrische reguläre Matrizen:} Theoretisch ist dieses Verfahren von \person{Paige} und \person{Saunders} identisch mit dem GMRES-Verfahren, jedoch gestattet die zusätzliche Symmetrie von $A$ wesentliche Vereinfachungen bei der Lösung der Teilprobleme.
	\item \textbf{BiCG-Verfahren für reguläre Matrizen:} Dieses Verfahren von \person{Lanczos} kann als Realisierung des \person{Galerkin} Basisalgorithmus (\propref{algorithmus_2_11}) angesehen werden, wobei die pro Schritt zu lösenden Teilprobleme dann durch
	\begin{lstlisting}
compute %$x^k\in x^0+\mathcal{K}_k(r^0,A)$ mit $b-Ax^k\perp\mathcal{K}_k(\bar{r}^0,A^T)$%
	\end{lstlisting}
	gegeben sind mit einem $\bar{r}^0$ derart, dass $(\bar{r}^0)^Tr^0\neq 0$.
	\item \textbf{CG-Verfahren als \person{Galerkin}-Verfahren:} Beim CG-Verfahren (\propref{algorithmus_2_9} mit $B=A^{-\sfrac{1}{2}}$ bzw. \propref{algorithmus_2_21}) ergibt sich aus \propref{lemma_2_14}
	\begin{align}
		(r^k)^Td^j=0\quad\text{für } j=0,...,k-1\notag
	\end{align}
	Außerdem hatten wir induktiv gezeigt (vgl. \cref{2_23}), dass $\mathcal{R}_k=\mathcal{K}_k(r^0,A)=\mathcal{D}_k$ gilt sofern $r^{k-1}\neq 0$. Also folgt
	\begin{align}
		b-Ax^k = r^k\perp \mathcal{K}_k(r^0,A)\notag
	\end{align}
	und weiter (vgl. die Teilprobleme in \propref{algorithmus_2_9})
	\begin{align}
		x^k\in x^0+\mathcal{K}_k(r^0,A)\quad\mit\quad b-Ax^k\perp \mathcal{K}_k(r^0,A)\notag
	\end{align}
	Damit kann man das CG-Verfahren auch als ein \person{Galerkin}-Verfahren betrachten.
	\item \textbf{Abbruchkriterien bei iterativen Lösern:} Anstelle der theoretischen Abbruchbedingung $b-Ax^k=0$ muss ein praktikableres Kriterium verwendet werden. Dazu bietet sich die Bedingung
	\begin{align}
		\frac{\norm{b-Ax^k}}{\norm{b-Ax^0}} \le \epsilon\notag
	\end{align}
	an das relative Residuum an, wobei $\epsilon>0$ vorzugeben ist. Zur Vermeidung einer unendlichen Anzahl von Iterationsschritten sind noch weitere Bedingungen (etwa an die maximale Iterationszahl oder die Differenz $x^k-x^{k-1}$) erforderlich.
\end{itemize}