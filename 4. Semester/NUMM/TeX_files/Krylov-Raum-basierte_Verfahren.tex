\section{\person{Krylov}-Raum-basierte Verfahren}

\subsection{\person{Krylov}-Räume}

Für $A\in\Rnn$, $r\in\Rn$ und $k\in\natur$ ist der $k$-te \begriff{Krylov-Raum} gegeben durch $\mathcal{K}_0=\{0\}$ und
\begin{align}
	\mathcal{K}_k(r,A) = \Span\{r,Ar,A^2r,...,A^{k-1}r\}\quad\text{ für } k>0\notag
\end{align}
Offenbar ist $\dim(\mathcal{K}_k(r,A))\le\min\{k,n\}$ für alle $k\in\natur$.

\begin{lemma}
	\proplbl{lemma_2_6}
	Es seien $A\in\Rnn$, $r\in\Rn\backslash\{0\}$ und $k\in\natur$ gegeben. Dann sind folgende Aussagen äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $\dim(\mathcal{K}_{k+1}(r,A))< k+1$
		\item $\mathcal{K}_k(r,A) = \mathcal{K}_{k+1}(r,A)$
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item (a) $\Rightarrow$ (b): Nach Voraussetzung gibt es $l\in\{1,...,k\}$ und $\alpha_0,...,\alpha_l\in\R$, so dass
		\begin{align}
			A^lr = \sum_{i=0}^{l-1} \alpha_iA^ir \notag
		\end{align}
		Multiplikation mit $A^{k-l}$ liefert
		\begin{align}
			A^kr = \sum_{i=0}^{l-1} \alpha_iA^{k-l+i}r\in\mathcal{K}_k(r,A)\notag
		\end{align}
		Also folgt $\mathcal{K}_k(r,A) = \mathcal{K}_{k+1}(r,A)$.
		\item (b) $\Rightarrow$ (a): Offensichtlich
	\end{itemize}
\end{proof}

\begin{proposition}
	\proplbl{satz_2_7}
	Es seien $A\in\Rnn$ regulär, $x^0\in\Rn$ mit $r^0=b-Ax^0\neq 0$ gegeben. Dann sind folgende Aussagen für $k\in\natur$ äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $\mathcal{K}_k(r^0,A) = \mathcal{K}_{k+1}(r^0,A)$
		\item $x^\ast = A^{-1}b\in x^0 + \mathcal{K}_k(r^0,A)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{itemize}
		\item (a) $\Rightarrow$ (b): Wegen \propref{lemma_2_6} gibt es $l\in\{0,...,k\}$ und $\mu_l,...,\mu_k\in\R$, so dass $\mu_l\neq 0$ und
		\begin{align}
			0 &= \sum_{i=l}^{k} \mu_iA^ir^0 \notag \\
			&= \mu_lA^lr^0 + \sum_{i=l+1}^{k} \mu_iA^ir^0 \notag
		\end{align}
		wobei der Summationsterm auf der rechten Seite entfällt, wenn $l=k$. Wegen der Regularität von $A$ kann man die Gleichung mit $A^{-(l+1)}$ multiplizieren. Für $l=k$ liefert dies $0=A^{-1}r^0 = A^{-1}b-x^0\in\mathcal{K}_k(r^0,A)$. Für $l<k$ folgt
		\begin{align}
			A^{-1}b - x^0 &= A^{-1}r \notag \\
			&= -\frac{1}{\mu_l} \sum_{i=l+1}^k \mu_iA^{i-l-1}r ^0 \notag \\
			&= -\frac{1}{\mu_l}\sum_{i=0}^{k-l-1} \mu_{i+l+1}A^ir^0\in\mathcal{K}_k(r^0,A) \notag
		\end{align}
		Somit gilt Aussage (b).
		\item (b) $\Rightarrow$ (a): Nach Voraussetzung gilt $x^\ast\in x_0 + \mathcal{K}_k(r^0,A)$. Durch Multiplikation mit $A$ folgt
		\begin{align}
			b&\in Ax^0 + A\mathcal{K}_k(r^0,A)\notag \\
			&= Ax^0 + \Span\{Ar^0,A^2r^0,A^3r^0,...,A^kr^0\}\notag
		\end{align}
		Also ist $r^0=b-Ax^0$ eine Linearkombination der Vektoren $Ar^0,A^2r^0,A^3r^0,...,A^kr^0$ und es gilt
		\begin{align}
			\dim(\mathcal{K}_{k+1}(r^0,A)) < k+1\notag
		\end{align}
		 \propref{lemma_2_6} liefert damit die Gültigkeit von Aussage (a).
	\end{itemize}
\end{proof}

\begin{remark}
	\proplbl{bemerkung_2_8}
	Offenbar gibt es $k^\ast\in\{1,...,n\}$, so dass Aussage (a) von \propref{satz_2_7} für $k^\ast$ zutrifft, aber für kein $k<k^\ast$ erfüllt ist. \propref{satz_2_7} zeigt daher, dass die exakte Lösung $x^\ast$ von $Ax=b$ in $x^0 + \mathcal{K}_{k^\ast}(r^0,A)$ liegt. Man kann also nun versuchen, eine Folge $\{x^k\}$ mit $x^k\in x^0 + \mathcal{K}_k(r^0,A)$ zu bestimmen, so dass $x^k$ das Gleichungssystem $Ax=b$ (geeignet) näherungsweise löst. Dazu werden in nächsten Abschnitt zwei grundlegende Ansätze angegeben (Minimum-Residuum und Galerkin).
\end{remark}

\subsection{Basisalgorithmen zur Lösung von $Ax=b$}

Für eine reguläre Matrix $B\in\Rnn$ ist durch
\begin{align}
	\norm{x}_B = \norm{Bx}_2\quad\text{für } x|in\Rn\notag
\end{align}
eine Vektornorm im $\Rn$ definiert. Unabhängig von der konkreten Wahl der regulären Matrix $B$ sind das lineare Gleichungssystem $Ax=b$ und die Minimierungs aufgabe
\begin{align}
	\frac{1}{2}\norm{b-Ax}_B^2\to\min\notag
\end{align}
offenbar äquivalent.

\begin{algorithm}[Minimum-Residuum Basisalgorithmus]
	\proplbl{algorithmus_2_9}
	Input: $x^0\in\R$, $A,B\in\Rnn$ regulär, $b\in\Rn$
\begin{lstlisting}
while %$r^k\neq 0$% do
 k = k + 1;
 compute %$x^k\in\Rn\text{ als Lösung von}$%
 %$\quad f_B(x) = \frac{1}{2}\Vert b-Ax\Vert^2_B\to\min\quad\text{bei } x\in x^0+\mathcal{K}_k(r^0,A)$%
 %$r^k$% = b - A%$x^k$%
enddo
\end{lstlisting}
	Output: $x^\ast=x^k$, $k^\ast=k$
\end{algorithm}

\begin{proposition}
	\propref{algorithmus_2_9} ist wohldefiniert und bricht (bei exakter Arithmetik) nach höchstens $n$ (das heißt $k^\ast\le n$) it der exakten Lösung $x^\ast$ des Gleichungssystems $Ax=b$ ab.
\end{proposition}
\begin{proof}
	Die Funktion $f_B:\Rn\to\R$ (Zielfunktion der Optimierungsaufgabe) ist gleichmäßig konvex, da $A$ regulär ist. Die Menge $x^0+\mathcal{K}_k(r^0,A)$ (zulässiger Bereich  der Optimierungsaufgabe) ist außerdem nichtleer, abgeschlossen und konvex.  Nach einem bekannten Resultat der kontinuierlichen Optimierung besitzt die Optimierungsaufgabe eine eindeutige Lösung $x^k$. Somit ist \propref{algorithmus_2_9} wohldefiniert. Wegen $\mathcal{K}_k(r^0,A)=\mathcal{K}_{k+1}(r^0,A)$ spätestens für $k=n$ muss \propref{algorithmus_2_9} auf Grund von \propref{satz_2_7} nach höchstens $n$ Schritten mit der exakten Lösung $x^\ast$ abbrechen, vgl. auch \propref{bemerkung_2_8}
\end{proof}

\propref{algorithmus_2_9} ist somit ein direktes Verfahren. Jedoch besteht die praktische Bedeutung von Realisierungen dieses Verfahrens darin, dass man unter bestimmten Bedingungen bereits für $k\ll k^\ast$ eine brauchbare Näherung $x^k$ für $x^\ast$ erhalten kann. Dabei ist es von entscheidender Bedeutung, dass jeder Schritt des Algorithmus möglichst wenig Aufwand erfordert (etwa $\mathcal{O}(n)$ für gewisse schwach besetzte Matrizen). Analoges trifft auch für folgende Klasse von Algorithmen zu.

\begin{algorithm}[Galerkin Basisalgorithmus]
	\proplbl{algorithmus_2_11}
	Input: $x^0\in\R$, $A,B\in\Rnn$ regulär, $b\in\Rn$
	\begin{lstlisting}
%$r^0$% = b - A%$x^0$%;
k = 0;
while %$r^k\neq 0$% do
 k = k + 1;
 determine %Untervektorraum $\mathcal{L}_k$ von $\Rn$ mit $\dim(\mathcal{L}_k)=k$%;
 compute %$x^k\in x^0+\mathcal{K}_k(r^0,A)$ mit $b-Ax^k\perp\mathcal{L}_k$%;
 %$r^k$% = b - A%$x^k$%;
enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$, $k^\ast=k$
\end{algorithm}

Unter der Voraussetzung der Durchführbarkeit bricht \propref{algorithmus_2_11} nach höchstens $n$ Schritten mit der Lösung $x^\ast$ von $Ax=b$ ab, da $\mathcal{L}_n=\Rn$ und $b-Ax^n\perp\mathcal{L}_n$ somit $b-Ax^n=0$ impliziert.

\subsection{Das CG-Verfahren}

Es seien $A\in\Rnn$ und $b\in\Rn$ gegeben. Die Funktion $f:\Rn\to\R$ sei gegeben durch
\begin{align}
	f(x) = \frac{1}{2}x^TAx-b^Tx\quad\text{für } x\in\Rn\notag
\end{align}
Falls die Matrix $A\in\Rnn$ symmetrisch und positiv definit ist (und das wird beim CG-Verfahren immer vorausgesetzt werden), gibt es eine orthogonale Matrix $Q\in\Rnn$ und eine Diagonalmatrix $\Lambda=\diag(\lambda_1,...,\lambda_n)$ mit den $n$ positiven Eigenwerten $\lambda_1,...,\lambda_n$ von $A$, so dass
\begin{align}
	A = Q\Lambda Q^T\notag
\end{align}
Mit $\Lambda^{\sfrac{1}{2}}=\diag(\sqrt{\lambda_1},...,\sqrt{\lambda_n})$ und $A^{\sfrac{1}{2}}=Q\Lambda^{\sfrac{1}{2}}Q^T$ gilt $A^{\sfrac{1}{2}}A^{\sfrac{1}{2}}=A$. Die Matrix $A^{\sfrac{1}{2}}$ wird als Wurzel von $A$ bezeichnet und ist symmetrisch und positiv definit. Es gibt keine weitere symmetrisch positiv definite Matrix, die Wurzel von $A$ ist.

\begin{lemma}
	\proplbl{lemma_2_12}
	Seien $A\in\Rnn$ symmetrisch und positiv definit, $b\in\Rn$, $G\subseteq\Rn$ eine abgeschlossene nichtleere Menge und $x^\ast=A^{-1}b$. Dann sind folgende Aussagen äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $y$ minimiert $f$ auf $G$
		\item $y$ minimiert $x\mapsto\norm{x-x^\ast}_{A^{\sfrac{1}{2}}}$ auf $G$
		\item $y$ minimiert $x\mapsto\norm{b-Ax}_{A^{-\sfrac{1}{2}}}$ auf $G$
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item (a) $\Rightarrow$ (b): Für jedes $x\in\Rn$ gilt
		\begin{align}
			\norm{x-x^\ast}^2_{A^{\sfrac{1}{2}}} &= \norm{A^{\sfrac{1}{2}}(x-x^\ast)}^2 \notag \\
			&= (x-x^\ast)^TA(x-x^\ast)\notag \\
			&= x^TAx-2x^TAx^\ast+(x^\ast)^TAx^\ast \notag \\
			&= x^TAx-2x^Tb+b^Tx^\ast\notag \\
			&= 2f(x) + b^Tx^\ast \notag
		\end{align}
		Da $b^Tx^\ast$ nicht von $x$ abhängt, ist eine Minimalstelle $y$ von $f(x)$ auf $G$ auch eine Minimalstelle von $x\mapsto\norm{x-x^\ast}_{A^{\sfrac{1}{2}}}$ auf $G$ und umgekehrt.
		\item (b) $\Rightarrow$ (c): Für jedes $x\in\Rnn$ gilt
		\begin{align}
			\norm{x-x^\ast}^2_{A^{\sfrac{1}{2}}} &= (x-x^\ast)^TA(x-x^\ast) \notag \\
			&= (x-x^\ast)^TAA^{-1}A(x-x^\ast) \notag \\
			&= (A(x-x^\ast))^TA^{-1}A(x-x^\ast) \notag \\
			&= (Ax-b)^TA^{-1}(Ax-b) \notag \\
			&= \norm{A^{-\sfrac{1}{2}}(b-Ax)}^2 \notag \\
			&= \norm{b-Ax}_{A^{-\sfrac{1}{2}}}^2 \notag
		\end{align}
		Damit folgt die Behauptung unmittelbar.
	\end{itemize}
\end{proof}

Das CG-Verfahren ergibt sich nn formal aus \propref{algorithmus_2_9} (Minimum-Residuum Basisalgorithmus), indem dort $B=A^{-\sfrac{1}{2}}$ gesetzt wird. \propref{lemma_2_12} zeigt uns dann, dass das Teilproblem, also
\begin{align}
	\label{2_13}
	\frac{1}{2}\norm{b-Ax}^2_{A^{-\sfrac{1}{2}}}\to\min\quad\text{bei } x\in x^0+\mathcal{K}_k(r^0,A)
\end{align}
zur Aufgabe
\begin{align}
	\label{2_14}
	\frac{1}{2}x^TAx-b^Tx\to\min\quad\text{bei } x\in x^0+\mathcal{K}_k(r^0,A)
\end{align}
äquivalent ist. Es stellt sich nun die Frage, wie diese Teilprobleme effizient zu lösen sind. Dazu gibt nachfolgendes Lemma einen Hinweis. Zu seiner Formulierung benötigen wir noch

\begin{definition}[$A$-konjugiert, $A$-orthogonal]
	Seien $A\in\Rnn$ symmetrisch und positiv definit und $k\in\natur$ mit $k\le n$. Dann nennt man Vektoren $d^0,...,d^{k-1}\in\Rn\backslash\{0\}$ mit der Eigenschaft
	\begin{align}
		(d^i)^TAd^j=0\quad\forall\text{ Paare } (i,j) \text{ mit } i,j\in\{0,...,k-1\} \text{ und } i\neq j\notag
	\end{align}
	\begriff{$A$-konjugiert} oder \begriff{$A$-orthogonal}.
\end{definition}

Die lineare Hülle von Vektoren $d^0,...,d^{k-1}\in\R^k$ soll durch 
\begin{align}
	\mathcal{D}_k=\Span\{d^0,...,d^{k-1}\}\notag
\end{align}
bezeichnet werden, wobei durch den Kontext klar ist, welche Vektoren $d^0,...,d^{k-1}$ gemeint sind.

\begin{lemma}
	\proplbl{lemma_2_14}
	Sei $A\in\Rnn$ symmetrisch und positiv definit. Weiter seien $x^0\in\Rn$ und $A$-konjugierte Vektoren $d^0,...,d^{n-1}\in\Rn\backslash\{0\}$ gegeben. Dann besitzt die Optimierungsaufgabe
	\begin{align}
		\label{2_15}
		\frac{1}{2}x^TAx-b^Tx\to\min\quad\text{bei } x\in x^0+\mathcal{D}_k
	\end{align}
	eine eindeutig bestimmte Lösung $x^k$ für $k=1,...,n$. Dabei gilt für $k=0,...,n-1$
	\begin{align}
		\label{2_16}
		x^{k+1} = x^k + t_kd^k\quad\mit\quad t_k = \frac{(r^k)^Td^k}{(d^k)^TAd^k}\quad\text{und}\quad r^k=b-Ax^k
	\end{align}
	und
	\begin{align}
		\label{2_17}
		(r^{k+1})^Td^j = 0\quad\text{für } j=0,...,k
	\end{align}
\end{lemma}

\begin{align}
	\label{2_23}
	\mathcal{R}_k = \mathcal{K}_k(r^0,A)=\mathcal{D}_k
\end{align}
\begin{proof}
	Zunächst sei die Funktion $\phi:\R^k\to\R$ für ein $k\in\{1,...,n\}$ wie folgt definiert:
	\begin{align}
		\phi(\alpha) = f\left(x ^0+\sum_{i=0}^{k-1}\alpha_id^i\right)\quad\forall\alpha\in\R^k\notag
	\end{align}
	Offenbar ist dann jede Lösung $\alpha^\ast$ der freien Optimierungsaufgabe 
	\begin{align}
		\label{2_18}
		\phi(\alpha)\to\min
	\end{align}
	vermöge
	\begin{align}
		\label{2_19}
		x^k = x^0+\sum_{i=0}^{k-1}\alpha^\ast_id^i
	\end{align}
	eine Lösung von \cref{2_15} und entsprechend umgekehrt. Wir untersuchen daher nun \cref{2_18}. Für $\phi(\alpha)$ erhält man
	\begin{align}
		\phi(\alpha) = \frac{1}{2}\left(x^0+\sum_{i=0}^{k-1}\alpha_id^i\right)^TA\left(x^0+\sum_{i=0}^{k-1}\alpha_id^i\right) - b^T\left(x^0+\sum_{i=0}^{k-1}\alpha_id^i\right)\notag
	\end{align}
	Da $d^0,...,d^{k-1}$ $A$-konjugiert sind, folgt
	\begin{align}
		\phi(\alpha) &= \frac{1}{2}\sum_{i=0}^{k-1}\alpha_i^2(d^i)^TAd^i + (Ax^0-b)^T\left(x^0 + \sum_{i=0}^{k-1}\alpha_i d^i\right) - \frac{1}{2}(x^0)^TAx^0 \notag \\
		&= \frac{1}{2}\sum_{i=0}^{k-1}\alpha_i^2(d^i)^TAd^i - \sum_{i=0}^{k-1}\alpha_i(r^0)^Td^i + f(x^0)\notag
	\end{align}
	Jede Lösung $\alpha^\ast$ von \cref{2_18} muss der notwendigen Optimalitätsbedingung $\nabla\phi(\alpha)=0$ genügen. Wegen der Konvexität von $\phi$ (beachte $\nabla^2\phi = A$ ist positiv definit) ist diese Bedingung auch hinreichend. Aus $\nabla\phi(\alpha^\ast)=0$ folgt
	\begin{align}
		0=\frac{\partial \phi}{\partial\alpha_i}(\alpha^\ast) =\alpha^\ast_i(d^i)^TAd^i-(r^0)^Td^i\quad\text{für } i=0,...,k-1\notag 
	\end{align}
	und damit (beachte positive Definitheit von $A$) nach Umstellen
	\begin{align}
		\label{2_20}
		\alpha_i^\ast = \frac{(r^0)^Td^i}{(d^i)^TAd^i}\quad\text{für } i=0,...,k-1
	\end{align}
	Also besitzt \cref{2_18} genau eine Lösung $\alpha^\ast$. Entsprechend \cref{2_15} eine eindeutige Lösung, die sich aus \cref{2_19} ergibt. Man sieht nun sofort, dass der Wert $\alpha_i^\ast$ nicht vom gewählten $k$ abhängig. Mit \cref{2_19} folgt daraus
	\begin{align}
		\label{2_21}
		x^{k+1} -x^k = \alpha_k^\ast d^k\quad\text{für } k=0,...,n-1
	\end{align}
	Damit erhält man für $i\in\{1,...,k-1\}$
	\begin{align}
		(r^0)^T d^i &= \left(r^i + \sum_{j=1}^{i} (r^{j-1}-r^j)\right)^Td^i \notag \\
		&= \left(r^i + \sum_{j=1}^i (-Ax^{j-1} + Ax^j)\right)^T d^i \notag \\
		&= (r^i)^Td^i + \sum_{j=1}^i (x^j-x^{j-1})^T Ad^i \notag \\
		&= (r^i)^Td^i + \sum_{j=1}^i \alpha^\ast_{j-1}(d^{j-1})^TAd^i \notag
	\end{align}
	und weiter unter Ausnutzung der $A$-Konjugiertheit der Vektoren $d^0,...,d^{n-1}$
	\begin{align}
		(r^0)^Td^i = (r^i)^Td^i\notag
	\end{align}
	Setzt man dies in die Darstellung von $\alpha_i^\ast$ ein ,so folgt wegen \cref{2_21} und \cref{2_20} Teilbehauptung \cref{2_16}.
	
	Die letzte Teilbehauptung \cref{2_17} ergibt sich für $j=k$ aus
	\begin{align}
		\label{2_22}
		\begin{split}
		(r^{k+1})^Td^k &= (b-Ax^{k+1})^Td^k \\
		&= (b-Ax^k-t_kAd^k)^Td^k \\
		&= (r^k)^Td^k-t_k(d^k)^TAd^k \\
		&= 0
		\end{split}
	\end{align}
	wobei die letzte Gleichung mit Hilfe der Darstellung von $t_k$ in \cref{2_16} klar wird. Für $j\in\{0,...,k-1\}$ zeigt man \cref{2_17} schließlich unter Benutzung von \cref{2_22} und der $A$-Konjugiertheit der $d^0,...,d^{n-1}$
	\begin{align}
		(r^{k+1})^Td^j &= (r^{j+1})^Td^j + \sum_{i=j+1}^k (r^{i+1}-r^i)^Td^j \notag \\
		&= \sum_{i=j+1}^k (b-Ax^{i+1}-b+Ax^i)^Td^j \notag \\
		&= \sum_{i=j+1}^k (-t_iAd^i)^Td^j \notag \\
		&= 0 \notag
	\end{align}
\end{proof}

\begin{algorithm}[CG-Verfahren]
	\proplbl{algorithmus_2_15}
	Input: $x^0\in\Rn$, $A\in\Rnn$ symmetrisch und positiv definit, $b\in\Rn$
	\begin{lstlisting}
%$d^0$% = %$r^0$% = b - A%$x^0$%;
k = 0;
while %$r^k\neq 0$% do
 %$t_k$% = %$\frac{\Vert r^k\Vert_2^2}{(d^k)^T Ad^k}$%;
 %$x^{k+1}$% = %$x^k$% + %$t_kd^k$%;
 %$r^{k+1}$% = b - A%$x^{k+1}$% = %$r^k$% - %$t_kAd^k$%;
 %$\beta_k$% = %$\frac{\Vert r^{k+1}\Vert_2^2}{\Vert r^k\Vert_2^2}$%;
 %$d^{k+1}$% = %$r^{k+1}$% - %$\beta_kd^k$%;
 k = k + 1;
enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$
\end{algorithm}

\subsection{Fehlerverhalten des CG-Verfahrens}

\subsection{Vorkonditionierung}

\begin{algorithm}[CG-Verfahren für $\tilde{A}\tilde{x}=\tilde{b}$]
	\proplbl{algorithmus_2_20}
	Input: $\tilde{x}^0\in\Rn$, $\tilde{A}\in\Rnn$ symmetrisch und positiv definit, $\tilde{b}\in\Rn$
	\begin{lstlisting}
	%$\tilde{d}^0$% = %$\tilde{r}^0$% = %$\tilde{b} - \tilde{A}\tilde{x}^0$%;
	k = 0;
	while %$\tilde{r}^k\neq 0$% do
	%$\tilde{t}_k$% = %$\frac{\Vert \tilde{r}^k\Vert_2^2}{(\tilde{d}^k)^T \tilde{A}\tilde{d}^k}$%;
	%$\tilde{x}^{k+1}$% = %$\tilde{x}^k$% + %$\tilde{t}_k\tilde{d}^k$%;
	%$\tilde{r}^{k+1}$% = %$\tilde{b} - \tilde{A}\tilde{x}^{k+1}$% = %$\tilde{r}^k$% - %$\tilde{t}_k\tilde{A}\tilde{d}^k$%;
	%$\tilde{\beta}_k$% = %$\frac{\Vert \tilde{r}^{k+1}\Vert_2^2}{\Vert \tilde{r}^k\Vert_2^2}$%;
	%$\tilde{d}^{k+1}$% = %$\tilde{r}^{k+1}$% - %$\tilde{\beta}_k\tilde{d}^k$%;
	k = k + 1;
	enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$
\end{algorithm}

\begin{algorithm}[Vorkonditioniertes CG-Verfahren]
	\proplbl{algorithmus_2_21}
	Input: $x^0\in\Rn$, $A\in\Rnn$ symmetrisch und positiv definit, $b\in\Rn$
	\begin{lstlisting}
%$r^0$% = b - A%$x^0$%;
compute %$z^0$ als Lösung von $Pz = r^0$% 
%$d^0$% = %$z^0$%
k = 0;
while %$r^k\neq 0$% do
 %$t_k$% = %$\frac{(r^k)^T}{(d^k)^T Ad^k}$%;
 %$x^{k+1}$% = %$x^k$% + %$t_kd^k$%;
 %$r^{k+1}$% = %$r^k$% - %$t_kAd^k$%;
 compute %$z^{k+1}$ als Lösung von $Pz = r^{k+1}$%
 %$\beta_k$% = %$\frac{(r^{k+1})^T z^{k+1}}{(r^k)^T z^k}$%;
 %$d^{k+1}$% = %$z^{k+1}$% + %$\beta_kd^k$%;
 k = k + 1;
enddo
	\end{lstlisting}
	Output: $x^\ast=x^k$
\end{algorithm}

\subsection{Ausblick und Anmerkungen}

Es gibt zahlreiche weitere sogenannte \person{Krylov}-Raum basierte Verfahren. Auf einige dieser Methoden wird hier kurz hingewiesen.
\begin{itemize}
	\item \textbf{CGNR-Verfahren für reguläre Matrizen:} Da $A$ regulär ist, muss das System $Ax=b$ äquivalent zu $A^TAx=A^Tb$ sein. Offenbar ist $A^TA$ eine symmetrische und positiv definite Matrix. Damit kann das CG-Verfahren prinzipiell auf $A^TAx=A^Tb$ angewendet werden. Dieses Vorgehen wird als CGNR-Verfahren bezeichnet, dabei gilt jedoch $\cond_2(A^TA)=\cond_2(A)^2$.
	\item \textbf{GMRES-Verfahren für reguläre Matrizen:} Das GMRES-Verfahren von \person{Saad} und \person{Schultz} kann dann als Spezialfall des Minimum-Residuum Basisalgorithmus (\propref{algorithmus_2_9}) aufgefasst werden, wenn dort $B=\mathbbm{1}$ gesetzt wird. Die dabei notwendige Erzeugung und Abspeicherung einer Orthonormalbasis der \person{Krylov}-Räume $\mathcal{K}_k(r^0,A)$ ist zu aufwendig, so dass man zu einem GMRES$(m)$-Verfahren übergeht, dass nach $m$ Schritten und einem Restart mit der letzten erhaltenen Iterierten als Startvektor durchführt und dieses Verfahren ggf. mehrfach wiederholt.
	\item \textbf{MINRES-Verfahren für symmetrische reguläre Matrizen:} Theoretisch ist dieses Verfahren von \person{Paige} und \person{Saunders} identisch mit dem GMRES-Verfahren, jedoch gestattet die zusätzliche Symmetrie von $A$ wesentliche Vereinfachungen bei der Lösung der Teilprobleme.
	\item \textbf{BiCG-Verfahren für reguläre Matrizen:} Dieses Verfahren von \person{Lanczos} kann als Realisierung des \person{Galerkin} Basisalgorithmus (\propref{algorithmus_2_11}) angesehen werden, wobei die pro Schritt zu lösenden Teilprobleme dann durch
	\begin{lstlisting}
compute %$x^k\in x^0+\mathcal{K}_k(r^0,A)$ mit $b-Ax^k\perp\mathcal{K}_k(\bar{r}^0,A^T)$%
	\end{lstlisting}
	gegeben sind mit einem $\bar{r}^0$ derart, dass $(\bar{r}^0)^Tr^0\neq 0$.
	\item \textbf{CG-Verfahren als \person{Galerkin}-Verfahren:} Beim CG-Verfahren (\propref{algorithmus_2_9} mit $B=A^{-\sfrac{1}{2}}$ bzw. \propref{algorithmus_2_21}) ergibt sich aus \propref{lemma_2_14}
	\begin{align}
		(r^k)^Td^j=0\quad\text{für } j=0,...,k-1\notag
	\end{align}
	Außerdem hatten wir induktiv gezeigt (vgl. \cref{2_23}), dass $\mathcal{R}_k=\mathcal{K}_k(r^0,A)=\mathcal{D}_k$ gilt sofern $r^{k-1}\neq 0$. Also folgt
	\begin{align}
		b-Ax^k = r^k\perp \mathcal{K}_k(r^0,A)\notag
	\end{align}
	und weiter (vgl. die Teilprobleme in \propref{algorithmus_2_9})
	\begin{align}
		x^k\in x^0+\mathcal{K}_k(r^0,A)\quad\mit\quad b-Ax^k\perp \mathcal{K}_k(r^0,A)\notag
	\end{align}
	Damit kann man das CG-Verfahren auch als ein \person{Galerkin}-Verfahren betrachten.
	\item \textbf{Abbruchkriterien bei iterativen Lösern:} Anstelle der theoretischen Abbruchbedingung $b-Ax^k=0$ muss ein praktikableres Kriterium verwendet werden. Dazu bietet sich die Bedingung
	\begin{align}
		\frac{\norm{b-Ax^k}}{\norm{b-Ax^0}} \le \epsilon\notag
	\end{align}
	an das relative Residuum an, wobei $\epsilon>0$ vorzugeben ist. Zur Vermeidung einer unendlichen Anzahl von Iterationsschritten sind noch weitere Bedingungen (etwa an die maximale Iterationszahl oder die Differenz $x^k-x^{k-1}$) erforderlich.
\end{itemize}